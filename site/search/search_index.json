{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Pinball","text":"<p>Fast, accurate quantile regression for Python.</p> <p> </p> <p> </p> <p>Pinball brings the speed and statistical rigor of R's quantreg package to Python, wrapped in a familiar scikit-learn interface.  It provides:</p> <ul> <li>High-performance Fortran solvers \u2014 the Barrodale-Roberts simplex and   Frisch-Newton interior-point algorithms, compiled from the original   quantreg Fortran code</li> <li>Preprocessing for massive datasets \u2014 the Portnoy-Koenker \"globs\"   technique that reduces million-row problems to manageable size</li> <li>Full statistical inference \u2014 standard errors, confidence intervals,   and three bootstrap methods (xy-pair, wild, MCMB)</li> <li>L1-penalised (lasso) quantile regression \u2014 for high-dimensional   sparse models using the Belloni-Chernozhukov approach</li> <li>Nonparametric conditional quantiles \u2014 the Charlier-Paindaveine-Saracco   quantization-based estimator for when linearity doesn't hold</li> <li>100% scikit-learn compatible \u2014 passes <code>check_estimator</code> for both   linear and nonparametric estimators</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from pinball import QuantileRegressor, load_engel\n\nX, y = load_engel(return_X_y=True)\nmodel = QuantileRegressor(tau=[0.1, 0.25, 0.5, 0.75, 0.9], method=\"fn\")\nmodel.fit(X, y)\nprint(model.coef_)\n</code></pre>"},{"location":"#why-pinball","title":"Why \"Pinball\"?","text":"<p>The pinball loss (also called the check function or asymmetric absolute loss) is the objective function at the heart of quantile regression:</p> \\[ \\rho_\\tau(u) = u \\cdot (\\tau - \\mathbf{1}_{u &lt; 0}) \\] <p>It penalises underestimates by \\(\\tau\\) and overestimates by \\(1 - \\tau\\), making it the natural loss for estimating the \\(\\tau\\)-th conditional quantile.</p>"},{"location":"#project-lineage","title":"Project Lineage","text":"<p>Pinball is a faithful port of two R packages:</p> Component R Source Reference Linear solvers &amp; inference quantreg Koenker (2005), Quantile Regression Preprocessing quantreg (<code>rq.fit.pfn</code>) Portnoy &amp; Koenker (1997), Statistical Science Nonparametric estimator QuantifQuantile Charlier, Paindaveine &amp; Saracco (2015), JSPI"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project are documented here.</p>"},{"location":"changelog/#010-unreleased","title":"0.1.0 (Unreleased)","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Linear quantile regression (<code>QuantileRegressor</code>)</li> <li>Barrodale-Roberts simplex solver (<code>\"br\"</code>) with rank-inversion CI</li> <li>Frisch-Newton interior-point solver (<code>\"fn\"</code> / <code>\"fnb\"</code>)</li> <li>Preprocessing + Frisch-Newton solver (<code>\"pfn\"</code>) for large datasets</li> <li>L1-penalised Lasso solver (<code>\"lasso\"</code>)</li> <li> <p>Extensible solver registry with <code>get_solver()</code> / <code>register_solver()</code></p> </li> <li> <p>Inference</p> </li> <li><code>summary()</code> with 5 SE methods: rank, iid, nid, ker, boot</li> <li> <p><code>bootstrap()</code> with 3 strategies: xy-pair, wild, mcmb</p> </li> <li> <p>Nonparametric estimation (<code>QuantizationQuantileEstimator</code>)</p> </li> <li>CLVQ optimal quantization grid construction</li> <li>Voronoi cell assignment and conditional quantile estimation</li> <li> <p>Bootstrap averaging over multiple grids</p> </li> <li> <p>sklearn compatibility</p> </li> <li>Full <code>check_estimator</code> compliance (46/46 for linear, 52/52 for nonparametric)</li> <li>Pipeline, cross-validation, and grid search support</li> <li> <p><code>BaseQuantileEstimator</code> abstract base class</p> </li> <li> <p>Datasets</p> </li> <li><code>load_engel()</code> \u2014 Engel food expenditure (235 obs, 1 predictor)</li> <li> <p><code>load_barro()</code> \u2014 Barro economic growth (161 obs, 13 predictors)</p> </li> <li> <p>Documentation</p> </li> <li>GitHub Pages site with MkDocs Material</li> <li>Theory guides, API reference, and example notebooks</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome!  Here's how to get started.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/yourorg/pinball.git\ncd pinball\n\n# Create a virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # macOS/Linux\n# .venv\\Scripts\\activate   # Windows\n\n# Install in editable mode with dev dependencies\npip install -e \".[dev]\" --no-build-isolation\n</code></pre>"},{"location":"contributing/#dependencies","title":"Dependencies","text":"<p>The Fortran extensions require a Fortran compiler.  On macOS:</p> <pre><code>brew install gcc\n</code></pre> <p>On Ubuntu/Debian:</p> <pre><code>sudo apt install gfortran\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest tests/ -v\n\n# Run with coverage\npytest tests/ --cov=pinball --cov-report=html\n\n# Run sklearn check_estimator compliance\npytest tests/test_sklearn_compat.py -v\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 guidelines</li> <li>Use type hints for function signatures</li> <li>Write docstrings in NumPy style</li> <li>Keep lines under 88 characters (Black default)</li> </ul>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch (<code>git checkout -b feature/my-feature</code>)</li> <li>Write tests for your changes</li> <li>Ensure all tests pass (<code>pytest tests/ -v</code>)</li> <li>Commit your changes (<code>git commit -m \"Add my feature\"</code>)</li> <li>Push to your fork (<code>git push origin feature/my-feature</code>)</li> <li>Open a Pull Request</li> </ol>"},{"location":"contributing/#adding-a-new-solver","title":"Adding a New Solver","text":"<p>The solver architecture uses the Open/Closed Principle \u2014 adding a new solver requires no changes to existing code:</p> <ol> <li>Create a new module in <code>pinball/linear/solvers/</code> (e.g. <code>my_solver.py</code>)</li> <li>Subclass <code>BaseSolver</code> and implement <code>_solve_impl()</code></li> <li>Register the solver in <code>pinball/linear/solvers/__init__.py</code>:</li> </ol> <pre><code>from pinball.linear.solvers.my_solver import MySolver\nregister_solver(\"my_method\", MySolver)\n</code></pre> <ol> <li>Add tests in <code>tests/test_solvers.py</code></li> <li>Update the documentation</li> </ol>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>Please include:</p> <ul> <li>Python version (<code>python --version</code>)</li> <li>NumPy/SciPy/sklearn versions (<code>pip list | grep -E \"numpy|scipy|scikit\"</code>)</li> <li>Operating system</li> <li>Minimal reproducible example</li> <li>Full traceback</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"license/","title":"License","text":"<p>Pinball is released under the MIT License.</p> <pre><code>MIT License\n\nCopyright (c) 2021, Michael Howard\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"api/datasets/","title":"Datasets","text":"<p>Pinball bundles two classic datasets used throughout the quantile regression literature.  Both are returned as <code>sklearn.utils.Bunch</code> objects for compatibility with sklearn workflows.</p>"},{"location":"api/datasets/#load_engel","title":"load_engel","text":"<pre><code>from pinball.datasets import load_engel\n</code></pre>"},{"location":"api/datasets/#description","title":"Description","text":"<p>Ernst Engel's 1857 food expenditure data.  235 Belgian working-class households with a single predictor (income) predicting food expenditure.  This is the canonical dataset for illustrating quantile regression \u2014 it appears in virtually every tutorial and textbook.</p>"},{"location":"api/datasets/#returns","title":"Returns","text":"Key Type Shape Description <code>data</code> <code>ndarray</code> <code>(235, 1)</code> Household income <code>target</code> <code>ndarray</code> <code>(235,)</code> Food expenditure <code>feature_names</code> <code>list[str]</code> \u2014 <code>[\"income\"]</code> <code>DESCR</code> <code>str</code> \u2014 Human-readable description"},{"location":"api/datasets/#example","title":"Example","text":"<pre><code>from pinball.datasets import load_engel\nfrom pinball import QuantileRegressor\n\nengel = load_engel()\nX, y = engel.data, engel.target\n\nmodel = QuantileRegressor(tau=[0.1, 0.25, 0.5 , 0.75, 0.9])\nmodel.fit(X, y)\n\n# The slopes diverge: wealthier households show more variability\n# in food spending \u2014 a textbook example of heteroscedasticity.\nprint(model.coef_)\n</code></pre>"},{"location":"api/datasets/#why-this-dataset","title":"Why this dataset?","text":"<p>Engel's law states that the proportion of income spent on food decreases as income rises.  Quantile regression reveals an additional insight: the variability of food spending also increases with income.  The slope at the 90th percentile is much steeper than at the 10th \u2014 richer households have more diverse food budgets.</p>"},{"location":"api/datasets/#load_barro","title":"load_barro","text":"<pre><code>from pinball.datasets import load_barro\n</code></pre>"},{"location":"api/datasets/#description_1","title":"Description","text":"<p>Cross-country economic growth data from Barro (1991) and Barro &amp; Lee (1994).  161 countries with 13 predictor variables and net GDP growth as the target.  This multi-predictor dataset is useful for demonstrating quantile regression with several covariates.</p>"},{"location":"api/datasets/#returns_1","title":"Returns","text":"Key Type Shape Description <code>data</code> <code>ndarray</code> <code>(161, 13)</code> Predictor variables <code>target</code> <code>ndarray</code> <code>(161,)</code> Net GDP growth rate <code>feature_names</code> <code>list[str]</code> \u2014 See table below <code>DESCR</code> <code>str</code> \u2014 Human-readable description"},{"location":"api/datasets/#feature-names","title":"Feature Names","text":"Column Name Description 0 <code>lgdp2</code> Log initial GDP per capita 1 <code>mse2</code> Male secondary school enrolment 2 <code>fse2</code> Female secondary school enrolment 3 <code>fhe2</code> Female higher education 4 <code>mhe2</code> Male higher education 5 <code>lexp2</code> Life expectancy 6 <code>lintr2</code> Log investment rate 7 <code>gedy2</code> Government education / GDP 8 <code>Iy2</code> Investment / GDP 9 <code>gcony2</code> Government consumption / GDP 10 <code>lblakp2</code> Log black-market premium 11 <code>pol2</code> Political instability 12 <code>ttrad2</code> Terms of trade"},{"location":"api/datasets/#example_1","title":"Example","text":"<pre><code>from pinball.datasets import load_barro\nfrom pinball import QuantileRegressor\n\nbarro = load_barro()\nX, y = barro.data, barro.target\n\n# Median regression\nmodel = QuantileRegressor(tau=0.5, method=\"fn\")\nmodel.fit(X, y)\n\n# Which predictors matter at the lower tail?\nmodel_low = QuantileRegressor(tau=0.1)\nmodel_low.fit(X, y)\nprint(dict(zip(barro.feature_names, model_low.coef_)))\n</code></pre>"},{"location":"api/datasets/#references","title":"References","text":"<ol> <li>Barro, R. (1991). \"Economic growth in a cross section of countries.\"    Quarterly Journal of Economics 106(2): 407\u2013443.</li> <li>Barro, R. and Lee, J.-W. (1994). \"Sources of economic growth.\"    Carnegie-Rochester Conference Series on Public Policy 40: 1\u201346.</li> </ol>"},{"location":"api/estimators/","title":"Estimators","text":""},{"location":"api/estimators/#basequantileestimator","title":"BaseQuantileEstimator","text":"<p>::: pinball.estimators.BaseQuantileEstimator</p> <p>Abstract base class for all conditional quantile estimators in pinball. Inherits from <code>sklearn.base.RegressorMixin</code> and <code>sklearn.base.BaseEstimator</code>.</p>"},{"location":"api/estimators/#interface","title":"Interface","text":"Method Description <code>fit(X, y)</code> Fit the model. Abstract \u2014 must be implemented by subclasses. <code>predict(X)</code> Predict conditional quantiles. Abstract. <code>score(X, y)</code> R\u00b2 score (inherited from <code>RegressorMixin</code>) <code>pinball_loss(X, y)</code> Mean pinball (check) loss: \\(\\frac{1}{n}\\sum_i \\rho_\\tau(y_i - \\hat y_i)\\)"},{"location":"api/estimators/#quantileregressor","title":"QuantileRegressor","text":"<pre><code>from pinball import QuantileRegressor\n</code></pre> <p>The primary estimator for linear quantile regression.  Wraps the high-performance Fortran solvers behind a familiar sklearn interface.</p>"},{"location":"api/estimators/#constructor","title":"Constructor","text":"<pre><code>QuantileRegressor(\n    tau=0.5,              # float or list of float\n    method=\"fn\",          # \"br\", \"fn\"/\"fnb\", \"pfn\", \"lasso\"\n    fit_intercept=True,   # bool\n    solver_options=None,  # dict or None\n)\n</code></pre>"},{"location":"api/estimators/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>tau</code> <code>float</code> or <code>list[float]</code> <code>0.5</code> Quantile level(s) in (0, 1). A list fits all quantiles simultaneously. <code>method</code> <code>str</code> <code>\"fn\"</code> Solver backend. See Linear Solvers. <code>fit_intercept</code> <code>bool</code> <code>True</code> Whether to add an intercept column. <code>solver_options</code> <code>dict</code> or <code>None</code> <code>None</code> Extra arguments forwarded to the solver (e.g. <code>{\"ci\": True}</code> for BR)."},{"location":"api/estimators/#attributes-after-fitting","title":"Attributes (after fitting)","text":"Attribute Shape Description <code>coef_</code> <code>(n_features,)</code> or <code>(n_features, n_quantiles)</code> Estimated coefficients (excluding intercept) <code>intercept_</code> <code>float</code> or <code>ndarray</code> Intercept term(s). Zero when <code>fit_intercept=False</code>. <code>residuals_</code> <code>(n_samples,)</code> or <code>(n_samples, n_quantiles)</code> Residuals from the fit <code>solver_result_</code> <code>SolverResult</code> or <code>list[SolverResult]</code> Full solver output for advanced use <code>n_features_in_</code> <code>int</code> Number of features seen during <code>fit</code> <code>n_iter_</code> <code>int</code> or <code>list[int]</code> Solver iteration count"},{"location":"api/estimators/#methods","title":"Methods","text":""},{"location":"api/estimators/#fitx-y-sample_weightnone","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit the quantile regression model.</p> Parameter Type Description <code>X</code> array-like, (n, p) Training data <code>y</code> array-like, (n,) Target values <code>sample_weight</code> array-like, (n,) or None Sample weights. Integer weights expand rows; float weights scale rows. <p>Returns <code>self</code>.</p>"},{"location":"api/estimators/#predictx","title":"<code>predict(X)</code>","text":"<p>Predict quantile(s) for new data.</p> Parameter Type Description <code>X</code> array-like, (n_new, p) New data <p>Returns <code>ndarray</code> of shape <code>(n_new,)</code> or <code>(n_new, n_quantiles)</code>.</p>"},{"location":"api/estimators/#pinball_lossx-y","title":"<code>pinball_loss(X, y)</code>","text":"<p>Mean pinball loss on the given data.</p>"},{"location":"api/estimators/#scorex-y","title":"<code>score(X, y)</code>","text":"<p>R\u00b2 score (inherited from <code>RegressorMixin</code>).</p>"},{"location":"api/estimators/#example","title":"Example","text":"<pre><code>import numpy as np\nfrom pinball import QuantileRegressor\n\nrng = np.random.default_rng(42)\nX = rng.normal(size=(500, 3))\ny = X @ [1, 2, 3] + rng.normal(size=500)\n\n# Single quantile\nmodel = QuantileRegressor(tau=0.5, method=\"fn\")\nmodel.fit(X, y)\nprint(model.coef_)       # \u2248 [1, 2, 3]\nprint(model.intercept_)  # \u2248 0\n\n# Multiple quantiles\nmodel = QuantileRegressor(tau=[0.1, 0.5, 0.9])\nmodel.fit(X, y)\nprint(model.coef_.shape)  # (3, 3) \u2014 one column per quantile\n</code></pre>"},{"location":"api/estimators/#quantizationquantileestimator","title":"QuantizationQuantileEstimator","text":"<pre><code>from pinball.nonparametric import QuantizationQuantileEstimator\n</code></pre> <p>Nonparametric conditional quantile estimator via optimal quantization (Charlier, Paindaveine &amp; Saracco, 2015).</p>"},{"location":"api/estimators/#constructor_1","title":"Constructor","text":"<pre><code>QuantizationQuantileEstimator(\n    tau=0.5,          # float\n    N=20,             # int \u2014 number of grid points\n    n_grids=50,       # int \u2014 bootstrap grids\n    p=2,              # float \u2014 L_p norm\n    random_state=None # int or None\n)\n</code></pre>"},{"location":"api/estimators/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>tau</code> <code>float</code> <code>0.5</code> Quantile level in (0, 1) <code>N</code> <code>int</code> <code>20</code> Number of points in the quantization grid <code>n_grids</code> <code>int</code> <code>50</code> Number of independent bootstrap grids (higher = more stable) <code>p</code> <code>float</code> <code>2</code> L_p norm exponent for CLVQ. <code>p=2</code> is Euclidean. <code>random_state</code> <code>int</code> or <code>None</code> <code>None</code> Random seed for reproducibility"},{"location":"api/estimators/#attributes-after-fitting_1","title":"Attributes (after fitting)","text":"Attribute Shape Description <code>grid_</code> <code>(N,)</code> or <code>(d, N)</code> Averaged optimal quantization grid <code>cell_quantiles_</code> <code>(N, 1)</code> Averaged conditional quantile per Voronoi cell <code>n_features_in_</code> <code>int</code> Number of features seen during <code>fit</code>"},{"location":"api/estimators/#methods_1","title":"Methods","text":""},{"location":"api/estimators/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Fit via CLVQ grid construction + Voronoi cell quantiles.</p> <p>Note</p> <p>This estimator does not accept <code>sample_weight</code> \u2014 the CLVQ algorithm's internal bootstrap depends on \\(n\\) in ways that make weighted and unweighted paths non-equivalent.</p>"},{"location":"api/estimators/#predictx_1","title":"<code>predict(X)</code>","text":"<p>Assign each row of <code>X</code> to its nearest Voronoi cell and return the cell's conditional quantile estimate.</p>"},{"location":"api/estimators/#example_1","title":"Example","text":"<pre><code>import numpy as np\nfrom pinball.nonparametric import QuantizationQuantileEstimator\n\nrng = np.random.default_rng(42)\nX = rng.uniform(0, 10, (500, 1))\ny = np.sin(X.ravel()) + 0.3 * rng.normal(size=500)\n\nmodel = QuantizationQuantileEstimator(tau=0.5, N=20, random_state=42)\nmodel.fit(X, y)\ny_hat = model.predict(X)\n</code></pre>"},{"location":"api/inference/","title":"Inference","text":"<p>Post-estimation inference for linear quantile regression: standard errors, confidence intervals, and bootstrap.</p>"},{"location":"api/inference/#summary-standard-errors","title":"Summary / Standard Errors","text":"<pre><code>from pinball import QuantileRegressor, summary\n</code></pre>"},{"location":"api/inference/#summarymodel-x-y-senid-alpha005","title":"<code>summary(model, X, y, se=\"nid\", alpha=0.05)</code>","text":"<p>Compute standard errors, t-statistics, p-values, and confidence intervals for a fitted <code>QuantileRegressor</code>.</p> Parameter Type Default Description <code>model</code> <code>QuantileRegressor</code> \u2014 A fitted model <code>X</code> array-like, (n, p) \u2014 Training data (same as used in <code>fit</code>) <code>y</code> array-like, (n,) \u2014 Training targets <code>se</code> <code>str</code> <code>\"nid\"</code> Standard error method (see below) <code>alpha</code> <code>float</code> <code>0.05</code> Significance level <p>Returns an <code>InferenceResult</code>.</p>"},{"location":"api/inference/#se-methods","title":"SE Methods","text":"Method Name Description Default for <code>\"rank\"</code> Rank inversion Koenker (1994) \u2014 exact CI based on dual BR solver only <code>\"iid\"</code> IID sandwich Koenker-Bassett (1978) \u2014 assumes i.i.d. errors \u2014 <code>\"nid\"</code> Huber sandwich Local sparsity / Hendricks-Koenker bandwidth \\(n \\geq 1001\\) <code>\"ker\"</code> Kernel sandwich Powell (1991) kernel estimate of the density \u2014 <code>\"boot\"</code> Bootstrap Delegates to <code>bootstrap()</code> \\(n &lt; 1001\\)"},{"location":"api/inference/#example","title":"Example","text":"<pre><code>from pinball import QuantileRegressor, summary\n\nmodel = QuantileRegressor(tau=0.5, method=\"fn\")\nmodel.fit(X, y)\n\nresult = summary(model, X, y, se=\"nid\")\nprint(result)\n# InferenceResult(se_method='nid')\n#                Coef    Std Err          t      P&gt;|t|     [0.025     0.975]\n#    intercept   81.48     14.63       5.57     0.0000      52.80    110.16\n#       income    0.56      0.01      38.41     0.0000       0.53      0.59\n</code></pre>"},{"location":"api/inference/#inferenceresult","title":"InferenceResult","text":"<pre><code>from pinball.linear._inference import InferenceResult\n</code></pre> <p>A dataclass holding the summary table.</p> Attribute Type Description <code>coefficients</code> <code>ndarray (p,)</code> Point estimates <code>std_errors</code> <code>ndarray (p,)</code> Standard errors <code>t_statistics</code> <code>ndarray (p,)</code> \\(t = \\text{coef} / \\text{se}\\) <code>p_values</code> <code>ndarray (p,)</code> Two-sided p-values <code>conf_int</code> <code>ndarray (p, 2)</code> Confidence interval bounds <code>se_method</code> <code>str</code> Which SE method was used <code>feature_names</code> <code>list[str]</code> or <code>None</code> Optional column names <p>The <code>__repr__</code> method prints a formatted table similar to R's <code>summary.rq()</code> output.</p>"},{"location":"api/inference/#bootstrap","title":"Bootstrap","text":"<pre><code>from pinball import bootstrap\n</code></pre>"},{"location":"api/inference/#bootstrapmodel-x-y-methodxy-pair-nboot200-alpha005-random_statenone","title":"<code>bootstrap(model, X, y, method=\"xy-pair\", nboot=200, alpha=0.05, random_state=None)</code>","text":"<p>Bootstrap inference using one of three strategies.</p> Parameter Type Default Description <code>model</code> <code>QuantileRegressor</code> \u2014 A fitted model <code>X</code> array-like, (n, p) \u2014 Training data <code>y</code> array-like, (n,) \u2014 Training targets <code>method</code> <code>str</code> <code>\"xy-pair\"</code> Bootstrap method (see below) <code>nboot</code> <code>int</code> <code>200</code> Number of bootstrap replicates <code>alpha</code> <code>float</code> <code>0.05</code> Significance level <code>random_state</code> <code>int</code> or <code>None</code> <code>None</code> Seed for reproducibility <p>Returns a <code>BootstrapResult</code>.</p>"},{"location":"api/inference/#bootstrap-methods","title":"Bootstrap Methods","text":"Method Name Description <code>\"xy-pair\"</code> XY-pair Classical nonparametric resampling of \\((x_i, y_i)\\) pairs (Efron, 1979) <code>\"wild\"</code> Wild bootstrap Perturbation-based bootstrap (Feng, He &amp; Hu, 2011) \u2014 better for heteroscedastic errors <code>\"mcmb\"</code> MCMB Markov chain marginal bootstrap (He &amp; Hu, 2002) \u2014 fastest, best for moderate \\(n\\)"},{"location":"api/inference/#example_1","title":"Example","text":"<pre><code>from pinball import QuantileRegressor, bootstrap\n\nmodel = QuantileRegressor(tau=0.5)\nmodel.fit(X, y)\n\nresult = bootstrap(model, X, y, method=\"xy-pair\", nboot=500, random_state=42)\nprint(result.std_errors)\nprint(result.conf_int)\n</code></pre>"},{"location":"api/inference/#bootstrapresult","title":"BootstrapResult","text":"<pre><code>from pinball.linear._bootstrap import BootstrapResult\n</code></pre> Attribute Type Description <code>boot_coefficients</code> <code>ndarray (nboot, p)</code> All bootstrap replicate coefficients <code>coefficients</code> <code>ndarray (p,)</code> Point estimate (original fit or mean of replicates) <code>std_errors</code> <code>ndarray (p,)</code> Column-wise std of <code>boot_coefficients</code> <code>conf_int</code> <code>ndarray (p, 2)</code> Percentile confidence intervals <code>bsmethod</code> <code>str</code> Bootstrap method used <code>nboot</code> <code>int</code> Number of replicates"},{"location":"api/inference/#properties","title":"Properties","text":"Property Description <code>boot_coefficients</code> The full R \u00d7 p matrix of replicate draws \u2014 useful for plotting bootstrap distributions"},{"location":"api/inference/#example-visualising-bootstrap-distribution","title":"Example: Visualising Bootstrap Distribution","text":"<pre><code>import matplotlib.pyplot as plt\n\nresult = bootstrap(model, X, y, nboot=1000, random_state=42)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\nfor j, ax in enumerate(axes):\n    ax.hist(result.boot_coefficients[:, j], bins=30, alpha=0.7)\n    ax.axvline(result.coefficients[j], color='red', linestyle='--')\n    ax.set_title(f\"\u03b2_{j}\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"api/inference/#references","title":"References","text":"<ol> <li>Koenker, R. and Bassett, G. (1978). \"Regression quantiles.\"    Econometrica 46(1): 33\u201350.</li> <li>Koenker, R. (1994). \"Confidence intervals for regression quantiles.\"</li> <li>Powell, J.L. (1991). \"Estimation of monotonic regression models    under quantile restrictions.\"</li> <li>Efron, B. (1979). \"Bootstrap methods: another look at the jackknife.\"</li> <li>Feng, X., He, X. and Hu, J. (2011). \"Wild bootstrap for quantile    regression.\" Biometrika 98(4): 995\u2013999.</li> <li>He, X. and Hu, F. (2002). \"Markov chain marginal bootstrap.\"    JASA 97(459): 783\u2013795.</li> </ol>"},{"location":"api/solvers/","title":"Solvers","text":"<p>The solver layer converts the quantile regression optimisation problem into a solution using one of several algorithms.  All solvers share a common interface (<code>BaseSolver</code>) and are accessed through a registry.</p>"},{"location":"api/solvers/#registry-functions","title":"Registry Functions","text":"<pre><code>from pinball.solvers import get_solver, list_solvers, register_solver\n</code></pre>"},{"location":"api/solvers/#get_solvername-kwargs","title":"<code>get_solver(name, **kwargs)</code>","text":"<p>Instantiate a solver by name.</p> <pre><code>solver = get_solver(\"fn\")          # FNBSolver with defaults\nsolver = get_solver(\"br\")          # BRSolver\nsolver = get_solver(\"lasso\", lambda_=0.1)\n</code></pre>"},{"location":"api/solvers/#list_solvers","title":"<code>list_solvers()</code>","text":"<p>Return a list of registered solver names.</p> <pre><code>&gt;&gt;&gt; list_solvers()\n['br', 'fn', 'fnb', 'lasso', 'pfn']\n</code></pre>"},{"location":"api/solvers/#register_solvername-cls","title":"<code>register_solver(name, cls)</code>","text":"<p>Register a custom solver class.</p> <pre><code>register_solver(\"my_solver\", MySolverClass)\n</code></pre>"},{"location":"api/solvers/#basesolver","title":"BaseSolver","text":"<pre><code>from pinball.solvers import BaseSolver\n</code></pre> <p>Abstract base class for all solvers.</p>"},{"location":"api/solvers/#interface","title":"Interface","text":"Method Description <code>solve(X, y, tau, **kwargs)</code> Validate inputs, solve, return <code>SolverResult</code> <code>validate_inputs(X, y, tau)</code> Input validation hook (override in subclasses) <code>_solve_impl(X, y, tau, **kwargs)</code> Abstract \u2014 the actual solve logic"},{"location":"api/solvers/#solvex-y-tau-kwargs-solverresult","title":"<code>solve(X, y, tau, **kwargs) \u2192 SolverResult</code>","text":"<p>The public entry point.  Calls <code>validate_inputs()</code> followed by <code>_solve_impl()</code>.</p>"},{"location":"api/solvers/#solverresult","title":"SolverResult","text":"<pre><code>from pinball.solvers import SolverResult\n</code></pre> <p>A dataclass returned by every solver.</p> Field Type Description <code>coefficients</code> <code>ndarray (p,)</code> Estimated coefficient vector <code>residuals</code> <code>ndarray (n,)</code> or <code>None</code> Residuals \\(y - X\\beta\\) <code>dual_solution</code> <code>ndarray (n,)</code> or <code>None</code> Dual variables (BR solver only) <code>objective_value</code> <code>float</code> or <code>None</code> Optimal pinball loss <code>status</code> <code>int</code> Solver exit status (0 = success) <code>iterations</code> <code>int</code> Number of iterations <code>solver_info</code> <code>dict</code> Additional solver-specific output"},{"location":"api/solvers/#brsolver","title":"BRSolver","text":"<pre><code>from pinball.linear.solvers.br import BRSolver\n</code></pre> <p>Barrodale-Roberts simplex solver.  Wraps the Fortran <code>rqbr</code> subroutine.</p>"},{"location":"api/solvers/#constructor","title":"Constructor","text":"<pre><code>BRSolver()  # no parameters\n</code></pre>"},{"location":"api/solvers/#solver-specific-options-via-kwargs-in-solve","title":"Solver-specific options (via <code>**kwargs</code> in <code>solve()</code>)","text":"Key Type Default Description <code>ci</code> <code>bool</code> <code>False</code> Compute rank-inversion confidence intervals <code>alpha</code> <code>float</code> <code>0.05</code> Significance level for CI <code>iid</code> <code>bool</code> <code>True</code> IID assumption for bandwidth in CI"},{"location":"api/solvers/#solver-info-keys","title":"Solver info keys","text":"<p>When <code>ci=True</code>, <code>solver_info</code> contains:</p> <ul> <li><code>\"ci\"</code> \u2014 confidence interval array, shape <code>(p, 2)</code></li> <li><code>\"ci_dual\"</code> \u2014 dual solutions at CI bounds</li> </ul>"},{"location":"api/solvers/#fnbsolver","title":"FNBSolver","text":"<pre><code>from pinball.linear.solvers.fnb import FNBSolver\n</code></pre> <p>Frisch-Newton interior-point solver (bounded variables formulation).</p>"},{"location":"api/solvers/#constructor_1","title":"Constructor","text":"<pre><code>FNBSolver(\n    beta=0.99995,  # step-size damping in (0, 1)\n    eps=1e-6,      # convergence tolerance\n)\n</code></pre>"},{"location":"api/solvers/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>beta</code> <code>float</code> <code>0.99995</code> Step-size damping. Larger = more aggressive. <code>eps</code> <code>float</code> <code>1e-6</code> Convergence tolerance. \\(\\tau\\) must be in \\((\\varepsilon, 1-\\varepsilon)\\)."},{"location":"api/solvers/#preprocessingsolver","title":"PreprocessingSolver","text":"<pre><code>from pinball.linear.solvers.pfn import PreprocessingSolver\n</code></pre> <p>Preprocessing + interior-point solver for large-\\(n\\) problems. See Preprocessing Theory for details.</p>"},{"location":"api/solvers/#constructor_2","title":"Constructor","text":"<pre><code>PreprocessingSolver(\n    inner_solver=None,     # BaseSolver or None (defaults to FNBSolver)\n    mm_factor=0.8,         # middle-band fraction\n    max_bad_fixups=3,      # fixup budget\n    eps=1e-6,              # bandwidth floor\n)\n</code></pre>"},{"location":"api/solvers/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>inner_solver</code> <code>BaseSolver</code> or <code>None</code> <code>FNBSolver()</code> Solver for the reduced subproblem <code>mm_factor</code> <code>float</code> <code>0.8</code> Fraction of subsample kept in the middle band <code>max_bad_fixups</code> <code>int</code> <code>3</code> Fixup iterations before doubling \\(m\\) <code>eps</code> <code>float</code> <code>1e-6</code> Floor for leverage-adjusted bandwidth"},{"location":"api/solvers/#lassosolver","title":"LassoSolver","text":"<pre><code>from pinball.linear.solvers.lasso import LassoSolver\n</code></pre> <p>\\(\\ell_1\\)-penalised quantile regression via augmented LP.</p>"},{"location":"api/solvers/#constructor_3","title":"Constructor","text":"<pre><code>LassoSolver(\n    lambda_=None,             # penalty (None = auto)\n    penalize_intercept=False, # penalise first column?\n    beta=0.99995,             # FNB damping\n    eps=1e-6,                 # FNB tolerance\n)\n</code></pre>"},{"location":"api/solvers/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>lambda_</code> <code>float</code> or <code>None</code> <code>None</code> Penalty parameter. <code>None</code> uses the Belloni-Chernozhukov default. <code>penalize_intercept</code> <code>bool</code> <code>False</code> Whether the intercept column is penalised <code>beta</code> <code>float</code> <code>0.99995</code> Interior-point damping <code>eps</code> <code>float</code> <code>1e-6</code> Convergence tolerance"},{"location":"api/solvers/#automatic-penalty","title":"Automatic penalty","text":"<p>When <code>lambda_=None</code>:</p> \\[ \\hat\\lambda = 2 \\, \\Phi^{-1}\\!\\left(1 - \\frac{0.05}{2p}\\right) \\sqrt{\\frac{\\tau(1-\\tau)}{n}} \\]"},{"location":"api/solvers/#direct-solver-usage","title":"Direct Solver Usage","text":"<p>All solvers can be used independently of the <code>QuantileRegressor</code> estimator:</p> <pre><code>import numpy as np\nfrom pinball.solvers import get_solver\n\nX = np.column_stack([np.ones(100), np.random.randn(100, 2)])\ny = np.random.randn(100)\n\nsolver = get_solver(\"fn\")\nresult = solver.solve(X, y, tau=0.5)\n\nprint(result.coefficients)\nprint(result.objective_value)\nprint(result.iterations)\n</code></pre> <p>Warning</p> <p>When using solvers directly, you must add the intercept column yourself if needed.  The <code>QuantileRegressor</code> adds it automatically when <code>fit_intercept=True</code>.</p>"},{"location":"examples/01_engel_curves/","title":"Engel Curves: A Classic Quantile Regression Example","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pinball import QuantileRegressor, summary\nfrom pinball.datasets import load_engel\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n</pre> import numpy as np import matplotlib.pyplot as plt from pinball import QuantileRegressor, summary from pinball.datasets import load_engel  plt.style.use('seaborn-v0_8-whitegrid') plt.rcParams['figure.figsize'] = (10, 6) In\u00a0[\u00a0]: Copied! <pre>engel = load_engel()\nX, y = engel.data, engel.target\nincome = X.ravel()\n\nprint(f'n = {len(y)}, features = {engel.feature_names}')\nprint(f'Income range: [{income.min():.0f}, {income.max():.0f}]')\nprint(f'Expenditure range: [{y.min():.0f}, {y.max():.0f}]')\n</pre> engel = load_engel() X, y = engel.data, engel.target income = X.ravel()  print(f'n = {len(y)}, features = {engel.feature_names}') print(f'Income range: [{income.min():.0f}, {income.max():.0f}]') print(f'Expenditure range: [{y.min():.0f}, {y.max():.0f}]') In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LinearRegression\n\nols = LinearRegression().fit(X, y)\nx_grid = np.linspace(income.min(), income.max(), 200).reshape(-1, 1)\n\nplt.scatter(income, y, alpha=0.4, s=20, label='Data')\nplt.plot(x_grid, ols.predict(x_grid), 'k--', lw=2, label='OLS (mean)')\nplt.xlabel('Household Income')\nplt.ylabel('Food Expenditure')\nplt.title('Engel Data \\u2014 OLS captures only the mean')\nplt.legend()\nplt.show()\n</pre> from sklearn.linear_model import LinearRegression  ols = LinearRegression().fit(X, y) x_grid = np.linspace(income.min(), income.max(), 200).reshape(-1, 1)  plt.scatter(income, y, alpha=0.4, s=20, label='Data') plt.plot(x_grid, ols.predict(x_grid), 'k--', lw=2, label='OLS (mean)') plt.xlabel('Household Income') plt.ylabel('Food Expenditure') plt.title('Engel Data \\u2014 OLS captures only the mean') plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>taus = [0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95]\ncolours = plt.cm.RdYlBu_r(np.linspace(0.1, 0.9, len(taus)))\n\nplt.figure(figsize=(10, 6))\nplt.scatter(income, y, alpha=0.3, s=15, color='grey')\n\nfor tau, col in zip(taus, colours):\n    model = QuantileRegressor(tau=tau, method='fn')\n    model.fit(X, y)\n    y_hat = model.predict(x_grid)\n    plt.plot(x_grid, y_hat, color=col, lw=2,\n             label=f'tau={tau:.2f} (slope={model.coef_[0]:.3f})')\n\nplt.xlabel('Household Income')\nplt.ylabel('Food Expenditure')\nplt.title('Quantile Regression Fan \\u2014 Engel Data')\nplt.legend(loc='upper left', fontsize=9)\nplt.show()\n</pre> taus = [0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95] colours = plt.cm.RdYlBu_r(np.linspace(0.1, 0.9, len(taus)))  plt.figure(figsize=(10, 6)) plt.scatter(income, y, alpha=0.3, s=15, color='grey')  for tau, col in zip(taus, colours):     model = QuantileRegressor(tau=tau, method='fn')     model.fit(X, y)     y_hat = model.predict(x_grid)     plt.plot(x_grid, y_hat, color=col, lw=2,              label=f'tau={tau:.2f} (slope={model.coef_[0]:.3f})')  plt.xlabel('Household Income') plt.ylabel('Food Expenditure') plt.title('Quantile Regression Fan \\u2014 Engel Data') plt.legend(loc='upper left', fontsize=9) plt.show() In\u00a0[\u00a0]: Copied! <pre>model = QuantileRegressor(tau=0.5, method='fn')\nmodel.fit(X, y)\n\nresult = summary(model, X, y, se='nid')\nprint(result)\n</pre> model = QuantileRegressor(tau=0.5, method='fn') model.fit(X, y)  result = summary(model, X, y, se='nid') print(result) In\u00a0[\u00a0]: Copied! <pre>slopes = []\nci_lo = []\nci_hi = []\n\nfor tau in taus:\n    model = QuantileRegressor(tau=tau, method='fn')\n    model.fit(X, y)\n    result = summary(model, X, y, se='nid')\n    slopes.append(result.coefficients[1])\n    ci_lo.append(result.conf_int[1, 0])\n    ci_hi.append(result.conf_int[1, 1])\n\nslopes = np.array(slopes)\nci_lo = np.array(ci_lo)\nci_hi = np.array(ci_hi)\n\nplt.figure(figsize=(8, 5))\nplt.fill_between(taus, ci_lo, ci_hi, alpha=0.2, color='steelblue')\nplt.plot(taus, slopes, 'o-', color='steelblue', lw=2, label='QR slope')\nplt.axhline(ols.coef_[0], color='black', ls='--', label='OLS slope')\nplt.xlabel('Quantile (tau)')\nplt.ylabel('Slope (income coefficient)')\nplt.title('Income Coefficient Across Quantiles')\nplt.legend()\nplt.show()\n</pre> slopes = [] ci_lo = [] ci_hi = []  for tau in taus:     model = QuantileRegressor(tau=tau, method='fn')     model.fit(X, y)     result = summary(model, X, y, se='nid')     slopes.append(result.coefficients[1])     ci_lo.append(result.conf_int[1, 0])     ci_hi.append(result.conf_int[1, 1])  slopes = np.array(slopes) ci_lo = np.array(ci_lo) ci_hi = np.array(ci_hi)  plt.figure(figsize=(8, 5)) plt.fill_between(taus, ci_lo, ci_hi, alpha=0.2, color='steelblue') plt.plot(taus, slopes, 'o-', color='steelblue', lw=2, label='QR slope') plt.axhline(ols.coef_[0], color='black', ls='--', label='OLS slope') plt.xlabel('Quantile (tau)') plt.ylabel('Slope (income coefficient)') plt.title('Income Coefficient Across Quantiles') plt.legend() plt.show() <p>The rising slope with tau confirms heteroscedasticity.</p> In\u00a0[\u00a0]: Copied! <pre>from pinball import bootstrap\n\nmodel = QuantileRegressor(tau=0.5, method='fn')\nmodel.fit(X, y)\n\nbs = bootstrap(model, X, y, method='xy-pair', nboot=500, random_state=42)\nprint(f'Bootstrap SE: {bs.std_errors}')\nprint(f'95% CI: {bs.conf_int}')\n\nplt.figure(figsize=(7, 4))\nplt.hist(bs.boot_coefficients[:, 1], bins=40, alpha=0.7, color='steelblue')\nplt.axvline(bs.coefficients[1], color='red', ls='--', lw=2, label='Point estimate')\nplt.xlabel('Income coefficient')\nplt.title('Bootstrap Distribution (500 xy-pair replicates)')\nplt.legend()\nplt.show()\n</pre> from pinball import bootstrap  model = QuantileRegressor(tau=0.5, method='fn') model.fit(X, y)  bs = bootstrap(model, X, y, method='xy-pair', nboot=500, random_state=42) print(f'Bootstrap SE: {bs.std_errors}') print(f'95% CI: {bs.conf_int}')  plt.figure(figsize=(7, 4)) plt.hist(bs.boot_coefficients[:, 1], bins=40, alpha=0.7, color='steelblue') plt.axvline(bs.coefficients[1], color='red', ls='--', lw=2, label='Point estimate') plt.xlabel('Income coefficient') plt.title('Bootstrap Distribution (500 xy-pair replicates)') plt.legend() plt.show()"},{"location":"examples/01_engel_curves/#engel-curves-a-classic-quantile-regression-example","title":"Engel Curves: A Classic Quantile Regression Example\u00b6","text":"<p>This notebook demonstrates quantile regression on Ernst Engel's 1857 data on food expenditure vs. household income.  We fit multiple quantiles to reveal heteroscedasticity \u2014 wealthier households show much more variability in food spending.</p>"},{"location":"examples/01_engel_curves/#load-the-data","title":"Load the Data\u00b6","text":""},{"location":"examples/01_engel_curves/#scatter-plot-with-ols","title":"Scatter Plot with OLS\u00b6","text":""},{"location":"examples/01_engel_curves/#fit-multiple-quantiles","title":"Fit Multiple Quantiles\u00b6","text":"<p>Quantile regression reveals how the entire distribution of food expenditure shifts with income.</p>"},{"location":"examples/01_engel_curves/#interpretation","title":"Interpretation\u00b6","text":"<p>The slopes diverge: higher quantiles have steeper slopes. This means:</p> <ul> <li>At the 10th percentile, each extra unit of income increases food expenditure modestly.</li> <li>At the 90th percentile, the same income increment raises food expenditure much more.</li> </ul> <p>OLS misses this entirely \\u2014 it reports a single slope for the conditional mean.</p>"},{"location":"examples/01_engel_curves/#inference-standard-errors-and-confidence-intervals","title":"Inference: Standard Errors and Confidence Intervals\u00b6","text":""},{"location":"examples/01_engel_curves/#coefficient-comparison-across-quantiles","title":"Coefficient Comparison Across Quantiles\u00b6","text":""},{"location":"examples/01_engel_curves/#bootstrap-inference","title":"Bootstrap Inference\u00b6","text":""},{"location":"examples/02_solver_comparison/","title":"Solver Comparison","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom pinball import QuantileRegressor\n\nplt.style.use('seaborn-v0_8-whitegrid')\nrng = np.random.default_rng(42)\n</pre> import numpy as np import time import matplotlib.pyplot as plt from pinball import QuantileRegressor  plt.style.use('seaborn-v0_8-whitegrid') rng = np.random.default_rng(42) In\u00a0[\u00a0]: Copied! <pre>def make_data(n, p=5, rng=rng):\n    X = rng.normal(size=(n, p))\n    beta_true = np.arange(1, p + 1, dtype=float)\n    noise = rng.normal(size=n) * (1 + np.abs(X[:, 0]))\n    y = X @ beta_true + noise\n    return X, y, beta_true\n\nX_test, y_test, _ = make_data(1000)\nprint(f'Shape: X={X_test.shape}, y={y_test.shape}')\n</pre> def make_data(n, p=5, rng=rng):     X = rng.normal(size=(n, p))     beta_true = np.arange(1, p + 1, dtype=float)     noise = rng.normal(size=n) * (1 + np.abs(X[:, 0]))     y = X @ beta_true + noise     return X, y, beta_true  X_test, y_test, _ = make_data(1000) print(f'Shape: X={X_test.shape}, y={y_test.shape}') In\u00a0[\u00a0]: Copied! <pre>sizes = [500, 1000, 2000, 5000, 10_000, 20_000, 50_000]\nmethods = ['br', 'fn', 'pfn']\ntimings = {m: [] for m in methods}\n\nfor n in sizes:\n    X, y, _ = make_data(n)\n    for method in methods:\n        model = QuantileRegressor(tau=0.5, method=method)\n        t0 = time.perf_counter()\n        model.fit(X, y)\n        elapsed = time.perf_counter() - t0\n        timings[method].append(elapsed)\n    print(f'n={n:&gt;6d}:  BR={timings[\"br\"][-1]:.4f}s  '\n          f'FN={timings[\"fn\"][-1]:.4f}s  PFN={timings[\"pfn\"][-1]:.4f}s')\n</pre> sizes = [500, 1000, 2000, 5000, 10_000, 20_000, 50_000] methods = ['br', 'fn', 'pfn'] timings = {m: [] for m in methods}  for n in sizes:     X, y, _ = make_data(n)     for method in methods:         model = QuantileRegressor(tau=0.5, method=method)         t0 = time.perf_counter()         model.fit(X, y)         elapsed = time.perf_counter() - t0         timings[method].append(elapsed)     print(f'n={n:&gt;6d}:  BR={timings[\"br\"][-1]:.4f}s  '           f'FN={timings[\"fn\"][-1]:.4f}s  PFN={timings[\"pfn\"][-1]:.4f}s') In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(9, 5))\nfor method, marker in zip(methods, ['o', 's', '^']):\n    plt.plot(sizes, timings[method], f'-{marker}', lw=2,\n             markersize=8, label=method.upper())\n\nplt.xlabel('n (samples)')\nplt.ylabel('Time (seconds)')\nplt.title('Solver Timing Comparison (p=5, tau=0.5)')\nplt.legend()\nplt.xscale('log')\nplt.yscale('log')\nplt.grid(True, which='both', alpha=0.3)\nplt.show()\n</pre> plt.figure(figsize=(9, 5)) for method, marker in zip(methods, ['o', 's', '^']):     plt.plot(sizes, timings[method], f'-{marker}', lw=2,              markersize=8, label=method.upper())  plt.xlabel('n (samples)') plt.ylabel('Time (seconds)') plt.title('Solver Timing Comparison (p=5, tau=0.5)') plt.legend() plt.xscale('log') plt.yscale('log') plt.grid(True, which='both', alpha=0.3) plt.show() In\u00a0[\u00a0]: Copied! <pre>X, y, beta_true = make_data(5000)\n\nprint(f'True beta: {beta_true}')\nprint()\n\nfor method in ['br', 'fn', 'pfn']:\n    model = QuantileRegressor(tau=0.5, method=method)\n    model.fit(X, y)\n    max_err = np.max(np.abs(model.coef_ - beta_true))\n    print(f'{method.upper():&gt;4s}: coef = {model.coef_}, '\n          f'max|error| = {max_err:.4f}')\n</pre> X, y, beta_true = make_data(5000)  print(f'True beta: {beta_true}') print()  for method in ['br', 'fn', 'pfn']:     model = QuantileRegressor(tau=0.5, method=method)     model.fit(X, y)     max_err = np.max(np.abs(model.coef_ - beta_true))     print(f'{method.upper():&gt;4s}: coef = {model.coef_}, '           f'max|error| = {max_err:.4f}') In\u00a0[\u00a0]: Copied! <pre>n, p = 500, 20\nX_sparse = rng.normal(size=(n, p))\nbeta_sparse = np.zeros(p)\nbeta_sparse[:3] = [2.0, -1.5, 1.0]\ny_sparse = X_sparse @ beta_sparse + rng.normal(size=n) * 0.5\n\nmodel_lasso = QuantileRegressor(tau=0.5, method='lasso')\nmodel_lasso.fit(X_sparse, y_sparse)\n\nmodel_fn = QuantileRegressor(tau=0.5, method='fn')\nmodel_fn.fit(X_sparse, y_sparse)\n\nplt.figure(figsize=(10, 5))\nx_pos = np.arange(p)\nwidth = 0.3\n\nplt.bar(x_pos - width, beta_sparse, width, label='True', alpha=0.8)\nplt.bar(x_pos, model_fn.coef_, width, label='FN (no penalty)', alpha=0.8)\nplt.bar(x_pos + width, model_lasso.coef_, width, label='Lasso', alpha=0.8)\n\nplt.xlabel('Feature index')\nplt.ylabel('Coefficient')\nplt.title('Lasso Quantile Regression \\u2014 Sparse Recovery')\nplt.legend()\nplt.xticks(x_pos)\nplt.show()\n\nprint(f'Non-zero Lasso coefficients: {np.sum(np.abs(model_lasso.coef_) &gt; 0.01)}')\n</pre> n, p = 500, 20 X_sparse = rng.normal(size=(n, p)) beta_sparse = np.zeros(p) beta_sparse[:3] = [2.0, -1.5, 1.0] y_sparse = X_sparse @ beta_sparse + rng.normal(size=n) * 0.5  model_lasso = QuantileRegressor(tau=0.5, method='lasso') model_lasso.fit(X_sparse, y_sparse)  model_fn = QuantileRegressor(tau=0.5, method='fn') model_fn.fit(X_sparse, y_sparse)  plt.figure(figsize=(10, 5)) x_pos = np.arange(p) width = 0.3  plt.bar(x_pos - width, beta_sparse, width, label='True', alpha=0.8) plt.bar(x_pos, model_fn.coef_, width, label='FN (no penalty)', alpha=0.8) plt.bar(x_pos + width, model_lasso.coef_, width, label='Lasso', alpha=0.8)  plt.xlabel('Feature index') plt.ylabel('Coefficient') plt.title('Lasso Quantile Regression \\u2014 Sparse Recovery') plt.legend() plt.xticks(x_pos) plt.show()  print(f'Non-zero Lasso coefficients: {np.sum(np.abs(model_lasso.coef_) &gt; 0.01)}') In\u00a0[\u00a0]: Copied! <pre>from pinball.datasets import load_engel\n\nengel = load_engel()\nX, y = engel.data, engel.target\n\nmodel = QuantileRegressor(tau=0.5, method='br',\n                          solver_options={'ci': True, 'alpha': 0.05})\nmodel.fit(X, y)\n\nci = model.solver_result_.solver_info.get('ci')\nif ci is not None:\n    print('Rank-inversion 95% CI:')\n    names = ['intercept'] + engel.feature_names\n    for i, name in enumerate(names):\n        print(f'  {name:&gt;12s}: [{ci[i, 0]:.4f}, {ci[i, 1]:.4f}]')\nelse:\n    print('CI not available in solver_info')\n</pre> from pinball.datasets import load_engel  engel = load_engel() X, y = engel.data, engel.target  model = QuantileRegressor(tau=0.5, method='br',                           solver_options={'ci': True, 'alpha': 0.05}) model.fit(X, y)  ci = model.solver_result_.solver_info.get('ci') if ci is not None:     print('Rank-inversion 95% CI:')     names = ['intercept'] + engel.feature_names     for i, name in enumerate(names):         print(f'  {name:&gt;12s}: [{ci[i, 0]:.4f}, {ci[i, 1]:.4f}]') else:     print('CI not available in solver_info')"},{"location":"examples/02_solver_comparison/#solver-comparison","title":"Solver Comparison\u00b6","text":"<p>Pinball ships four linear quantile regression solvers.  This notebook compares them on problems of increasing size.</p> Solver Method key Best for Barrodale-Roberts <code>br</code> n &lt; 5,000 Frisch-Newton <code>fn</code> 5,000 &lt; n &lt; 100,000 Preprocessing + FN <code>pfn</code> n &gt; 100,000 Lasso <code>lasso</code> Sparse / high-dimensional"},{"location":"examples/02_solver_comparison/#generate-synthetic-data","title":"Generate Synthetic Data\u00b6","text":""},{"location":"examples/02_solver_comparison/#timing-br-vs-fn-vs-pfn","title":"Timing: BR vs FN vs PFN\u00b6","text":""},{"location":"examples/02_solver_comparison/#key-observations","title":"Key Observations\u00b6","text":"<ul> <li>BR (simplex) is fastest for small n but grows superlinearly.</li> <li>FN (interior point) scales better and dominates for medium n.</li> <li>PFN (preprocessing) has startup overhead but wins for large n.</li> </ul>"},{"location":"examples/02_solver_comparison/#accuracy-comparison","title":"Accuracy Comparison\u00b6","text":""},{"location":"examples/02_solver_comparison/#lasso-sparse-feature-selection","title":"Lasso: Sparse Feature Selection\u00b6","text":""},{"location":"examples/02_solver_comparison/#br-solver-with-confidence-intervals","title":"BR Solver with Confidence Intervals\u00b6","text":""},{"location":"examples/03_nonparametric_quantization/","title":"Nonparametric Quantile Estimation via Quantization","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom pinball import QuantileRegressor\nfrom pinball.nonparametric import QuantizationQuantileEstimator\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\nrng = np.random.default_rng(42)\n</pre> import numpy as np import matplotlib.pyplot as plt from pinball import QuantileRegressor from pinball.nonparametric import QuantizationQuantileEstimator  plt.style.use('seaborn-v0_8-whitegrid') plt.rcParams['figure.figsize'] = (10, 6) rng = np.random.default_rng(42) In\u00a0[\u00a0]: Copied! <pre>n = 800\nX_1d = rng.uniform(0, 2 * np.pi, n)\nnoise_scale = 0.3 * (1 + 0.5 * X_1d)\ny = np.sin(X_1d) + noise_scale * rng.normal(size=n)\n\nX = X_1d.reshape(-1, 1)\norder = np.argsort(X_1d)\n\nplt.scatter(X_1d, y, alpha=0.3, s=10, color='grey')\nplt.plot(X_1d[order], np.sin(X_1d[order]), 'k-', lw=2, label='True mean')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Nonlinear Data with Heteroscedastic Noise')\nplt.legend()\nplt.show()\n</pre> n = 800 X_1d = rng.uniform(0, 2 * np.pi, n) noise_scale = 0.3 * (1 + 0.5 * X_1d) y = np.sin(X_1d) + noise_scale * rng.normal(size=n)  X = X_1d.reshape(-1, 1) order = np.argsort(X_1d)  plt.scatter(X_1d, y, alpha=0.3, s=10, color='grey') plt.plot(X_1d[order], np.sin(X_1d[order]), 'k-', lw=2, label='True mean') plt.xlabel('X') plt.ylabel('Y') plt.title('Nonlinear Data with Heteroscedastic Noise') plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>linear_model = QuantileRegressor(tau=0.5, method='fn')\nlinear_model.fit(X, y)\n\nnp_model = QuantizationQuantileEstimator(\n    tau=0.5, N=30, n_grids=50, random_state=42\n)\nnp_model.fit(X, y)\n\nx_grid = np.linspace(0, 2 * np.pi, 300).reshape(-1, 1)\ny_linear = linear_model.predict(x_grid)\ny_np = np_model.predict(x_grid)\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X_1d, y, alpha=0.2, s=10, color='grey', label='Data')\nplt.plot(x_grid, np.sin(x_grid), 'k-', lw=2, label='True median')\nplt.plot(x_grid, y_linear, '--', color='tab:red', lw=2, label='Linear QR')\nplt.plot(x_grid, y_np, '-', color='tab:blue', lw=2, label='Quantization QR')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Median Regression: Linear vs. Nonparametric')\nplt.legend()\nplt.show()\n</pre> linear_model = QuantileRegressor(tau=0.5, method='fn') linear_model.fit(X, y)  np_model = QuantizationQuantileEstimator(     tau=0.5, N=30, n_grids=50, random_state=42 ) np_model.fit(X, y)  x_grid = np.linspace(0, 2 * np.pi, 300).reshape(-1, 1) y_linear = linear_model.predict(x_grid) y_np = np_model.predict(x_grid)  plt.figure(figsize=(10, 6)) plt.scatter(X_1d, y, alpha=0.2, s=10, color='grey', label='Data') plt.plot(x_grid, np.sin(x_grid), 'k-', lw=2, label='True median') plt.plot(x_grid, y_linear, '--', color='tab:red', lw=2, label='Linear QR') plt.plot(x_grid, y_np, '-', color='tab:blue', lw=2, label='Quantization QR') plt.xlabel('X') plt.ylabel('Y') plt.title('Median Regression: Linear vs. Nonparametric') plt.legend() plt.show() <p>The linear model fits a straight line (missing the curvature), while the quantization estimator tracks the sinusoidal pattern.</p> In\u00a0[\u00a0]: Copied! <pre>taus = [0.1, 0.25, 0.5, 0.75, 0.9]\ncolours = plt.cm.RdYlBu_r(np.linspace(0.1, 0.9, len(taus)))\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X_1d, y, alpha=0.15, s=8, color='grey')\n\nfor tau, col in zip(taus, colours):\n    model = QuantizationQuantileEstimator(\n        tau=tau, N=30, n_grids=50, random_state=42\n    )\n    model.fit(X, y)\n    y_hat = model.predict(x_grid)\n    plt.plot(x_grid, y_hat, color=col, lw=2, label=f'tau = {tau}')\n\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Nonparametric Quantile Regression \\u2014 Multiple Quantiles')\nplt.legend()\nplt.show()\n</pre> taus = [0.1, 0.25, 0.5, 0.75, 0.9] colours = plt.cm.RdYlBu_r(np.linspace(0.1, 0.9, len(taus)))  plt.figure(figsize=(10, 6)) plt.scatter(X_1d, y, alpha=0.15, s=8, color='grey')  for tau, col in zip(taus, colours):     model = QuantizationQuantileEstimator(         tau=tau, N=30, n_grids=50, random_state=42     )     model.fit(X, y)     y_hat = model.predict(x_grid)     plt.plot(x_grid, y_hat, color=col, lw=2, label=f'tau = {tau}')  plt.xlabel('X') plt.ylabel('Y') plt.title('Nonparametric Quantile Regression \\u2014 Multiple Quantiles') plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n\nfor ax, N in zip(axes, [5, 20, 50]):\n    model = QuantizationQuantileEstimator(\n        tau=0.5, N=N, n_grids=50, random_state=42\n    )\n    model.fit(X, y)\n    y_hat = model.predict(x_grid)\n\n    ax.scatter(X_1d, y, alpha=0.15, s=5, color='grey')\n    ax.plot(x_grid, np.sin(x_grid), 'k-', lw=1.5, label='Truth')\n    ax.plot(x_grid, y_hat, '-', color='tab:blue', lw=2, label=f'N={N}')\n    ax.set_title(f'N = {N}')\n    ax.set_xlabel('X')\n    ax.legend(fontsize=9)\n\naxes[0].set_ylabel('Y')\nplt.suptitle('Effect of Grid Size N on the Quantization Estimator', y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)  for ax, N in zip(axes, [5, 20, 50]):     model = QuantizationQuantileEstimator(         tau=0.5, N=N, n_grids=50, random_state=42     )     model.fit(X, y)     y_hat = model.predict(x_grid)      ax.scatter(X_1d, y, alpha=0.15, s=5, color='grey')     ax.plot(x_grid, np.sin(x_grid), 'k-', lw=1.5, label='Truth')     ax.plot(x_grid, y_hat, '-', color='tab:blue', lw=2, label=f'N={N}')     ax.set_title(f'N = {N}')     ax.set_xlabel('X')     ax.legend(fontsize=9)  axes[0].set_ylabel('Y') plt.suptitle('Effect of Grid Size N on the Quantization Estimator', y=1.02) plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\npipe = Pipeline([\n    ('scale', StandardScaler()),\n    ('quant', QuantizationQuantileEstimator(tau=0.5, N=25, random_state=42)),\n])\n\nscores = cross_val_score(pipe, X, y, cv=5, scoring='r2')\nprint(f'5-fold CV R-squared scores: {scores}')\nprint(f'Mean R-squared = {scores.mean():.3f} +/- {scores.std():.3f}')\n</pre> from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.model_selection import cross_val_score  pipe = Pipeline([     ('scale', StandardScaler()),     ('quant', QuantizationQuantileEstimator(tau=0.5, N=25, random_state=42)), ])  scores = cross_val_score(pipe, X, y, cv=5, scoring='r2') print(f'5-fold CV R-squared scores: {scores}') print(f'Mean R-squared = {scores.mean():.3f} +/- {scores.std():.3f}') In\u00a0[\u00a0]: Copied! <pre>n2 = 1000\nX2 = rng.uniform(0, 2 * np.pi, (n2, 2))\ny2 = np.sin(X2[:, 0]) * np.cos(X2[:, 1]) + 0.3 * rng.normal(size=n2)\n\nmodel_2d = QuantizationQuantileEstimator(\n    tau=0.5, N=40, n_grids=50, random_state=42\n)\nmodel_2d.fit(X2, y2)\n\nx1_grid = np.linspace(0, 2 * np.pi, 50)\nx2_grid = np.linspace(0, 2 * np.pi, 50)\nX1g, X2g = np.meshgrid(x1_grid, x2_grid)\nX_pred = np.column_stack([X1g.ravel(), X2g.ravel()])\nY_pred = model_2d.predict(X_pred).reshape(X1g.shape)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nY_true = np.sin(X1g) * np.cos(X2g)\nim0 = axes[0].contourf(X1g, X2g, Y_true, levels=20, cmap='RdYlBu_r')\naxes[0].set_title('True Median Surface')\naxes[0].set_xlabel('X1')\naxes[0].set_ylabel('X2')\nplt.colorbar(im0, ax=axes[0])\n\nim1 = axes[1].contourf(X1g, X2g, Y_pred, levels=20, cmap='RdYlBu_r')\naxes[1].set_title('Estimated Median (N=40, B=50)')\naxes[1].set_xlabel('X1')\naxes[1].set_ylabel('X2')\nplt.colorbar(im1, ax=axes[1])\n\nplt.suptitle('2D Nonparametric Quantile Regression', y=1.02)\nplt.tight_layout()\nplt.show()\n</pre> n2 = 1000 X2 = rng.uniform(0, 2 * np.pi, (n2, 2)) y2 = np.sin(X2[:, 0]) * np.cos(X2[:, 1]) + 0.3 * rng.normal(size=n2)  model_2d = QuantizationQuantileEstimator(     tau=0.5, N=40, n_grids=50, random_state=42 ) model_2d.fit(X2, y2)  x1_grid = np.linspace(0, 2 * np.pi, 50) x2_grid = np.linspace(0, 2 * np.pi, 50) X1g, X2g = np.meshgrid(x1_grid, x2_grid) X_pred = np.column_stack([X1g.ravel(), X2g.ravel()]) Y_pred = model_2d.predict(X_pred).reshape(X1g.shape)  fig, axes = plt.subplots(1, 2, figsize=(14, 5))  Y_true = np.sin(X1g) * np.cos(X2g) im0 = axes[0].contourf(X1g, X2g, Y_true, levels=20, cmap='RdYlBu_r') axes[0].set_title('True Median Surface') axes[0].set_xlabel('X1') axes[0].set_ylabel('X2') plt.colorbar(im0, ax=axes[0])  im1 = axes[1].contourf(X1g, X2g, Y_pred, levels=20, cmap='RdYlBu_r') axes[1].set_title('Estimated Median (N=40, B=50)') axes[1].set_xlabel('X1') axes[1].set_ylabel('X2') plt.colorbar(im1, ax=axes[1])  plt.suptitle('2D Nonparametric Quantile Regression', y=1.02) plt.tight_layout() plt.show()"},{"location":"examples/03_nonparametric_quantization/#nonparametric-quantile-estimation-via-quantization","title":"Nonparametric Quantile Estimation via Quantization\u00b6","text":"<p>Linear quantile regression assumes the conditional quantile is a linear function of the covariates.  The <code>QuantizationQuantileEstimator</code> provides a fully nonparametric alternative using optimal quantization.</p> <p>Reference: Charlier, Paindaveine &amp; Saracco (2015). JSPI 156, 14\\u201330.</p>"},{"location":"examples/03_nonparametric_quantization/#synthetic-nonlinear-data","title":"Synthetic Nonlinear Data\u00b6","text":"<p>We generate data from a sinusoidal model with heteroscedastic noise.</p>"},{"location":"examples/03_nonparametric_quantization/#linear-vs-nonparametric-median-regression","title":"Linear vs. Nonparametric: Median Regression\u00b6","text":""},{"location":"examples/03_nonparametric_quantization/#multiple-quantiles-with-the-nonparametric-estimator","title":"Multiple Quantiles with the Nonparametric Estimator\u00b6","text":""},{"location":"examples/03_nonparametric_quantization/#effect-of-grid-size-n","title":"Effect of Grid Size N\u00b6","text":""},{"location":"examples/03_nonparametric_quantization/#using-in-an-sklearn-pipeline","title":"Using in an sklearn Pipeline\u00b6","text":""},{"location":"examples/03_nonparametric_quantization/#2d-example-bivariate-covariates","title":"2D Example: Bivariate Covariates\u00b6","text":""},{"location":"examples/03_nonparametric_quantization/#summary","title":"Summary\u00b6","text":"Feature Linear QR Quantization QR Functional form Linear Any Coefficients Yes No (cell-based) Standard errors Yes No Curse of dimensionality No Yes Best for Interpretable linear effects Nonlinear patterns"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#from-pypi-recommended","title":"From PyPI (recommended)","text":"<pre><code>pip install pinball\n</code></pre>"},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>Pinball includes compiled Fortran extensions, so building from source requires a Fortran compiler (e.g. <code>gfortran</code>) and the Meson build system.</p> <pre><code>git clone https://github.com/mah38900/pinball.git\ncd pinball\npython -m venv .venv\nsource .venv/bin/activate\npip install meson-python meson ninja numpy\npip install -e . --no-build-isolation\n</code></pre>"},{"location":"getting-started/installation/#macos","title":"macOS","text":"<pre><code>brew install gcc  # provides gfortran\n</code></pre>"},{"location":"getting-started/installation/#ubuntu-debian","title":"Ubuntu / Debian","text":"<pre><code>sudo apt install gfortran\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"Package Version Purpose NumPy \u2265 1.23 Array operations SciPy \u2265 1.10 Statistical distributions, sparse linear algebra scikit-learn \u2265 1.3 Base estimator classes, validation utilities"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import pinball\nprint(pinball.__version__)\n\n# Quick smoke test \u2014 fit the Engel dataset\nfrom pinball import QuantileRegressor, load_engel\nX, y = load_engel(return_X_y=True)\nmodel = QuantileRegressor(tau=0.5).fit(X, y)\nprint(f\"Intercept: {model.intercept_:.2f}, Slope: {model.coef_[0]:.4f}\")\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#fitting-your-first-quantile-regression","title":"Fitting Your First Quantile Regression","text":"<pre><code>import numpy as np\nfrom pinball import QuantileRegressor\n\n# Generate some data\nrng = np.random.default_rng(42)\nn = 500\nX = rng.uniform(-2, 2, (n, 1))\ny = 2 * X.ravel() + 1 + rng.standard_t(df=3, size=n)\n\n# Fit the median (tau=0.5)\nmodel = QuantileRegressor(tau=0.5, method=\"fn\")\nmodel.fit(X, y)\nprint(f\"Intercept: {model.intercept_:.3f}\")\nprint(f\"Slope:     {model.coef_[0]:.3f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#multiple-quantiles-at-once","title":"Multiple Quantiles at Once","text":"<p>Pinball can fit several quantile levels in a single call:</p> <pre><code>model = QuantileRegressor(tau=[0.1, 0.25, 0.5, 0.75, 0.9])\nmodel.fit(X, y)\n\n# coef_ has shape (n_features, n_quantiles)\nfor i, tau in enumerate([0.1, 0.25, 0.5, 0.75, 0.9]):\n    print(f\"  tau={tau:.2f}  intercept={model.intercept_[i]:.3f}  \"\n          f\"slope={model.coef_[0, i]:.3f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#inference-standard-errors-and-confidence-intervals","title":"Inference: Standard Errors and Confidence Intervals","text":"<pre><code>from pinball import summary\n\nresult = summary(model, X, y, se_method=\"nid\")\nprint(result)\n</code></pre>"},{"location":"getting-started/quickstart/#bootstrap-confidence-intervals","title":"Bootstrap Confidence Intervals","text":"<pre><code>from pinball import bootstrap\n\nboot = bootstrap(model, X, y, nboot=500, method=\"xy\")\nprint(boot)\n</code></pre>"},{"location":"getting-started/quickstart/#choosing-a-solver","title":"Choosing a Solver","text":"Method Solver Best for Complexity <code>\"br\"</code> Barrodale-Roberts simplex Small data (n &lt; 5,000), exact CI \\(O(n \\cdot p^2)\\) <code>\"fn\"</code> Frisch-Newton interior point Medium data (default) \\(O(n \\cdot p^{1.5})\\) <code>\"pfn\"</code> Preprocessing + Frisch-Newton Large data (n &gt; 50,000) \\(\\tilde{O}(\\sqrt{p} \\cdot n^{2/3})\\) <code>\"lasso\"</code> L1-penalised interior point High-dimensional sparse \\(O(n \\cdot p^{1.5})\\) <pre><code># For a dataset with 1 million rows\nmodel = QuantileRegressor(tau=0.5, method=\"pfn\")\nmodel.fit(X_large, y_large)\n</code></pre>"},{"location":"getting-started/quickstart/#nonparametric-estimation","title":"Nonparametric Estimation","text":"<p>When the conditional quantile function is non-linear and you don't want to specify a parametric form:</p> <pre><code>from pinball.nonparametric.quantization import QuantizationQuantileEstimator\n\nest = QuantizationQuantileEstimator(tau=0.5, N=20, n_grids=50, random_state=42)\nest.fit(X, y)\ny_pred = est.predict(X_new)\n</code></pre>"},{"location":"getting-started/quickstart/#using-with-scikit-learn-pipelines","title":"Using with scikit-learn Pipelines","text":"<p>Both estimators are fully sklearn-compatible:</p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"qr\", QuantileRegressor(tau=0.5, method=\"fn\")),\n])\npipe.fit(X, y)\n</code></pre>"},{"location":"theory/linear-methods/","title":"Linear Solvers","text":"<p>Pinball ships four linear quantile-regression solvers.  Each targets a different regime of problem size and structure.  They all solve the same linear program but differ in computational complexity, auxiliary output, and numerical behaviour.</p>"},{"location":"theory/linear-methods/#solver-selection-guide","title":"Solver Selection Guide","text":"Solver Method key Best for Complexity CI built-in? Barrodale-Roberts <code>\"br\"</code> \\(n \\lesssim 5\\,000\\) \\(O(n \\cdot p^2)\\) worst case Yes (rank-inversion) Frisch-Newton <code>\"fn\"</code> / <code>\"fnb\"</code> \\(5\\,000 &lt; n &lt; 100\\,000\\) \\(O(n \\cdot p^{1.5})\\) typical No Preprocessing + FN <code>\"pfn\"</code> \\(n &gt; 100\\,000\\) \\(O(\\sqrt{p} \\cdot n^{2/3} \\cdot p^{1.5})\\) No Lasso <code>\"lasso\"</code> Sparse / high-dimensional same as FN on augmented problem No <pre><code>from pinball import QuantileRegressor\n\n# Choose the solver via the `method` parameter\nmodel = QuantileRegressor(tau=0.5, method=\"fn\")\nmodel.fit(X, y)\n</code></pre>"},{"location":"theory/linear-methods/#barrodale-roberts-br","title":"Barrodale-Roberts (<code>\"br\"</code>)","text":"<p>The simplex-based solver of Barrodale &amp; Roberts (1974), as specialised for quantile regression by Koenker &amp; d'Orey (1987, 1994).  This is the workhorse of small-sample quantile regression and remains the default in R's <code>quantreg::rq()</code>.</p>"},{"location":"theory/linear-methods/#how-it-works","title":"How it works","text":"<p>The check-function minimisation is reformulated as a linear program. The Barrodale-Roberts algorithm is a modified simplex method that exploits the special structure of the constraint matrix (every row of \\(X\\) enters at most one basis) to achieve very efficient pivoting.</p> <p>Each simplex iteration corresponds to an interpolation: the quantile hyperplane passes through \\(p\\) data points.  The algorithm moves from one set of interpolation points to the next, always decreasing the objective.</p>"},{"location":"theory/linear-methods/#unique-features","title":"Unique features","text":"<ul> <li> <p>Dual solution \u2014 the solver returns the full dual vector \\(d_i\\),   which classifies every observation as above, below, or on the   hyperplane.  This is needed for rank-inversion confidence intervals.</p> </li> <li> <p>Rank-inversion CI \u2014 when called with <code>ci=True</code>, the solver   computes Koenker (1994) confidence intervals by inverting a rank   test.  These intervals are exact (up to discretisation) and do not   require density estimation.</p> </li> <li> <p>Full quantile process \u2014 with <code>tau=None</code>, the solver computes   the solution path for all quantile levels where the solution   changes, which is a piecewise-linear function of \\(\\tau\\).</p> </li> </ul>"},{"location":"theory/linear-methods/#when-to-use","title":"When to use","text":"<ul> <li>Small datasets (\\(n \\lesssim 5\\,000\\)); exact CI are needed; or you   want the full quantile process.</li> </ul>"},{"location":"theory/linear-methods/#example","title":"Example","text":"<pre><code>from pinball import QuantileRegressor\n\nmodel = QuantileRegressor(tau=0.5, method=\"br\",\n                          solver_options={\"ci\": True, \"alpha\": 0.05})\nmodel.fit(X, y)\n# solver_result_ contains dual solution and CI\nci = model.solver_result_.solver_info[\"ci\"]\n</code></pre>"},{"location":"theory/linear-methods/#references","title":"References","text":"<ol> <li>Barrodale, I. and Roberts, F.D.K. (1974). \"Solution of an    overdetermined system of equations in the \\(\\ell_1\\) norm.\"</li> <li>Koenker, R. and d'Orey, V. (1987, 1994). \"Computing regression    quantiles.\" Applied Statistics.</li> <li>Koenker, R. (1994). \"Confidence intervals for regression quantiles.\"</li> </ol>"},{"location":"theory/linear-methods/#frisch-newton-interior-point-fn-fnb","title":"Frisch-Newton Interior Point (<code>\"fn\"</code> / <code>\"fnb\"</code>)","text":"<p>An interior-point (barrier) method that solves the quantile regression LP by following a central path through the interior of the feasible region.  This is the algorithm described in Portnoy &amp; Koenker (1997) as the \"Laplacian tortoise\" \u2014 slower per iteration than simplex, but with far better scaling.</p>"},{"location":"theory/linear-methods/#how-it-works_1","title":"How it works","text":"<p>The primal-dual LP is solved by Newton steps on the KKT system with a log-barrier term.  Each iteration solves a \\(p \\times p\\) linear system (not \\(n \\times n\\)), so the cost per iteration is \\(O(n p^2)\\) for a dense Cholesky factorisation.  Convergence is typically achieved in \\(O(\\sqrt{n})\\) iterations, giving an overall complexity of \\(O(n^{3/2} p^2)\\) \u2014 much better than simplex for large \\(n\\).</p>"},{"location":"theory/linear-methods/#parameters","title":"Parameters","text":"Parameter Default Description <code>beta</code> 0.99995 Step-size damping; larger values take more aggressive steps <code>eps</code> 1e-6 Convergence tolerance; \\(\\tau\\) must be in \\((\\varepsilon, 1-\\varepsilon)\\)"},{"location":"theory/linear-methods/#when-to-use_1","title":"When to use","text":"<ul> <li>Medium-to-large datasets (\\(5\\,000 &lt; n &lt; 100\\,000\\))</li> <li>When CI can be computed separately (via <code>summary()</code> or <code>bootstrap()</code>)</li> <li>As the default for most practical work</li> </ul>"},{"location":"theory/linear-methods/#example_1","title":"Example","text":"<pre><code>from pinball import QuantileRegressor\n\nmodel = QuantileRegressor(tau=0.5, method=\"fn\")\nmodel.fit(X, y)\n</code></pre>"},{"location":"theory/linear-methods/#references_1","title":"References","text":"<ol> <li>Portnoy, S. and Koenker, R. (1997). \"The Gaussian hare and the    Laplacian tortoise.\" Statistical Science 12(4): 279\u2013300.</li> </ol>"},{"location":"theory/linear-methods/#preprocessing-frisch-newton-pfn","title":"Preprocessing + Frisch-Newton (<code>\"pfn\"</code>)","text":"<p>The key to scaling quantile regression to really large datasets. This solver wraps the Frisch-Newton method with the Portnoy-Koenker preprocessing strategy, reducing the effective problem size from \\(n\\) to \\(O(\\sqrt{p} \\cdot n^{2/3})\\).</p> The key insight <p>Most observations are far from the quantile hyperplane and can be aggregated into two summary points (one for observations above, one for below) without changing the optimal solution.  Only the \"middle band\" of observations near the hyperplane participates in each LP iteration.</p> <p>This is the solver for datasets with \\(n &gt; 100{,}000\\) \u2014 or even millions of rows.  See the dedicated preprocessing section for the full theory.</p>"},{"location":"theory/linear-methods/#parameters_1","title":"Parameters","text":"Parameter Default Description <code>inner_solver</code> <code>FNBSolver()</code> Any <code>BaseSolver</code> used on the reduced subproblem <code>mm_factor</code> 0.8 Fraction of subsample \\(m\\) kept in the \"middle band\" <code>max_bad_fixups</code> 3 Fixup iterations before doubling \\(m\\) <code>eps</code> 1e-6 Bandwidth floor for detecting extreme residuals"},{"location":"theory/linear-methods/#when-to-use_2","title":"When to use","text":"<ul> <li>Large datasets (\\(n &gt; 100{,}000\\))</li> <li>Any problem where FNB is too slow</li> </ul>"},{"location":"theory/linear-methods/#example_2","title":"Example","text":"<pre><code>from pinball import QuantileRegressor\n\nmodel = QuantileRegressor(tau=0.5, method=\"pfn\")\nmodel.fit(X_large, y_large)\n</code></pre>"},{"location":"theory/linear-methods/#references_2","title":"References","text":"<ol> <li>Portnoy, S. and Koenker, R. (1997). \"The Gaussian hare and the    Laplacian tortoise.\" Statistical Science 12(4): 279\u2013300.</li> </ol>"},{"location":"theory/linear-methods/#ell_1-penalised-lasso-lasso","title":"\\(\\ell_1\\)-Penalised Lasso (<code>\"lasso\"</code>)","text":"<p>Sparse quantile regression via \\(\\ell_1\\) penalisation.  Solves:</p> \\[ \\hat\\beta(\\tau) = \\arg\\min_{\\beta} \\sum_{i=1}^{n} \\rho_\\tau(y_i - x_i^\\top \\beta) + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\] <p>The penalised problem is converted to an augmented LP and solved by the Frisch-Newton interior-point method.</p>"},{"location":"theory/linear-methods/#how-it-works_2","title":"How it works","text":"<p>The penalty is incorporated by augmenting the design matrix:</p> \\[ X_{\\text{aug}} = \\begin{bmatrix} X \\\\ \\text{diag}(\\lambda) \\end{bmatrix}, \\quad y_{\\text{aug}} = \\begin{bmatrix} y \\\\ 0 \\end{bmatrix} \\] <p>This converts the penalised problem into an unpenalised quantile regression on a larger dataset, which is then solved by FNB.</p>"},{"location":"theory/linear-methods/#parameters_2","title":"Parameters","text":"Parameter Default Description <code>lambda_</code> <code>None</code> (auto) Penalty parameter; if <code>None</code>, Belloni-Chernozhukov default is used <code>penalize_intercept</code> <code>False</code> Whether the intercept column is penalised"},{"location":"theory/linear-methods/#automatic-penalty-selection","title":"Automatic penalty selection","text":"<p>When <code>lambda_=None</code>, the solver uses the Belloni &amp; Chernozhukov (2011) pivot-based default:</p> \\[ \\hat\\lambda = 2 \\cdot \\Phi^{-1}\\!\\left(1 - \\frac{\\alpha}{2p}\\right) \\cdot \\sqrt{\\frac{\\tau(1-\\tau)}{n}} \\]"},{"location":"theory/linear-methods/#when-to-use_3","title":"When to use","text":"<ul> <li>High-dimensional settings where \\(p\\) is large relative to \\(n\\)</li> <li>When many coefficients are expected to be zero (sparsity)</li> <li>Variable-selection applications</li> </ul>"},{"location":"theory/linear-methods/#example_3","title":"Example","text":"<pre><code>from pinball import QuantileRegressor\n\n# Automatic lambda\nmodel = QuantileRegressor(tau=0.5, method=\"lasso\")\nmodel.fit(X_sparse, y)\n\n# Custom lambda\nmodel = QuantileRegressor(tau=0.5, method=\"lasso\",\n                          solver_options={\"lambda_\": 0.1})\nmodel.fit(X_sparse, y)\n</code></pre>"},{"location":"theory/linear-methods/#references_3","title":"References","text":"<ol> <li>Belloni, A. and Chernozhukov, V. (2011). \"\\(\\ell_1\\)-penalized quantile    regression in high-dimensional sparse models.\" Annals of Statistics.</li> </ol>"},{"location":"theory/linear-methods/#solver-registry","title":"Solver Registry","text":"<p>All solvers are registered in a global dictionary and can be listed programmatically:</p> <pre><code>from pinball.solvers import list_solvers, get_solver\n\n# See available methods\nprint(list_solvers())\n# ['br', 'fn', 'fnb', 'lasso', 'pfn']\n\n# Instantiate directly\nsolver = get_solver(\"fn\")\nresult = solver.solve(X, y, tau=0.5)\n</code></pre> <p>Custom solvers can be registered with <code>register_solver()</code>:</p> <pre><code>from pinball.solvers import register_solver, BaseSolver\n\nclass MySolver(BaseSolver):\n    def _solve_impl(self, X, y, tau, **kwargs):\n        ...\n\nregister_solver(\"my_method\", MySolver)\n</code></pre>"},{"location":"theory/nonparametric/","title":"Nonparametric Quantile Estimation via Optimal Quantization","text":""},{"location":"theory/nonparametric/#beyond-linearity","title":"Beyond Linearity","text":"<p>The linear quantile regression model \\(Q_\\tau(Y \\mid X) = X^\\top \\beta(\\tau)\\) is flexible and interpretable, but it assumes the conditional quantile is a linear function of the covariates.  When the relationship is nonlinear \u2014 threshold effects, saturation, interactions \u2014 a different approach is needed.</p> <p>The quantization-based estimator of Charlier, Paindaveine &amp; Saracco (2015) provides a fully nonparametric alternative.  It makes no assumption about the functional form of \\(Q_\\tau(Y \\mid X = x)\\) and works by:</p> <ol> <li>Partitioning the covariate space into Voronoi cells</li> <li>Estimating the conditional quantile within each cell from the data</li> <li>Averaging over multiple random partitions for stability</li> </ol>"},{"location":"theory/nonparametric/#optimal-quantization","title":"Optimal Quantization","text":"<p>Quantization approximates a continuous distribution by a discrete one with \\(N\\) support points, chosen to minimise the expected \\(L_p\\) distance between a random vector and its nearest grid point:</p> \\[ \\hat\\gamma_N = \\arg\\min_{\\gamma = \\{c_1, \\dots, c_N\\}} E\\left[\\min_{j} \\|X - c_j\\|^p\\right] \\] <p>This is the same idea as \\(k\\)-means clustering (\\(p = 2\\)) but applied to probability distributions rather than datasets.  The grid points \\(c_1, \\dots, c_N\\) are called codebook vectors or quantization centroids.</p>"},{"location":"theory/nonparametric/#clvq-algorithm","title":"CLVQ Algorithm","text":"<p>The Competitive Learning Vector Quantization (CLVQ) algorithm constructs the optimal grid via stochastic gradient descent.  Starting from random initial positions, it processes observations one at a time:</p> <ol> <li>Draw a stimulus \\(x\\) from the data (with replacement)</li> <li>Find the nearest grid point \\(c_{j^*}\\)</li> <li>Move \\(c_{j^*}\\) toward \\(x\\) with a decaying step size:</li> </ol> \\[ c_{j^*} \\leftarrow c_{j^*} + \\gamma_t \\cdot \\|x - c_{j^*}\\|^{p-2} (x - c_{j^*}) \\] <p>The step size \\(\\gamma_t\\) decays as \\(\\gamma_0 \\cdot a / (a + \\gamma_0 \\cdot b \\cdot t)\\) with \\(a = 4N\\) and \\(b = \\pi^2 N^{-2}\\), ensuring convergence while allowing early exploration.</p> <p>After one full pass through the data, the grid points have converged to an \\(L_p\\)-optimal quantization of the covariate distribution.</p>"},{"location":"theory/nonparametric/#voronoi-tessellation","title":"Voronoi Tessellation","text":"<p>The optimal grid induces a Voronoi partition of the covariate space.  Each observation \\((x_i, y_i)\\) is assigned to the cell of its nearest grid point:</p> \\[ \\text{cell}(x_i) = \\arg\\min_{j \\in \\{1, \\dots, N\\}} \\|x_i - c_j\\| \\] <p>Within each cell \\(j\\), the \\(\\tau\\)-quantile of \\(Y\\) is estimated from the \\(y_i\\) values of observations in that cell:</p> \\[ \\hat Q_\\tau^{(j)} = \\text{quantile}_\\tau\\{y_i : x_i \\in \\text{cell}_j\\} \\] <p>This gives a piecewise-constant estimate of the conditional quantile function: within each Voronoi cell, the prediction is the cell's sample quantile.</p>"},{"location":"theory/nonparametric/#bootstrap-averaging","title":"Bootstrap Averaging","text":"<p>A single CLVQ grid is random (it depends on the bootstrap sample and the initial grid positions).  To obtain stable estimates, the algorithm constructs \\(B\\) independent grids and averages the cell-level quantile estimates.</p> <p>For each grid \\(b = 1, \\dots, B\\):</p> <ol> <li>Bootstrap a sample of size \\(n\\) from the data</li> <li>Run CLVQ to obtain grid \\(\\gamma^{(b)}\\)</li> <li>Assign all \\(n\\) training observations to cells</li> <li>Compute cell quantiles \\(\\hat Q_\\tau^{(b,j)}\\)</li> </ol> <p>The final grid is the centroid of all \\(B\\) grids:</p> \\[ \\bar\\gamma = \\frac{1}{B} \\sum_{b=1}^{B} \\gamma^{(b)} \\] <p>and the final cell quantiles are averaged across the \\(B\\) grids.</p> <p>At prediction time, a new \\(x\\) is assigned to its nearest cell in \\(\\bar\\gamma\\) and the corresponding averaged quantile is returned.</p>"},{"location":"theory/nonparametric/#algorithm-summary","title":"Algorithm Summary","text":"<pre><code>graph TD\n    A[\"Data (X, Y)\"] --&gt; B[\"Bootstrap sample b\"]\n    B --&gt; C[\"CLVQ \u2192 grid \u03b3&lt;sup&gt;(b)&lt;/sup&gt;\"]\n    C --&gt; D[\"Voronoi assign\"]\n    D --&gt; E[\"Cell quantiles Q\u0302&lt;sub&gt;\u03c4&lt;/sub&gt;&lt;sup&gt;(b,j)&lt;/sup&gt;\"]\n    E --&gt; F{\"b &lt; B?\"}\n    F --&gt;|Yes| B\n    F --&gt;|No| G[\"Average grids &amp; quantiles\"]\n    G --&gt; H[\"Predict: assign x \u2192 nearest cell\"]\n</code></pre>"},{"location":"theory/nonparametric/#properties","title":"Properties","text":""},{"location":"theory/nonparametric/#advantages","title":"Advantages","text":"<ul> <li>No functional form assumption \u2014 the conditional quantile can be   any function of \\(X\\), including discontinuous or non-monotonic</li> <li>Works in any dimension \u2014 the CLVQ algorithm and Voronoi   partition generalise naturally to \\(d\\)-dimensional covariates</li> <li>Bootstrap averaging gives smooth, stable estimates despite   the stochastic grid construction</li> <li>Interpretable \u2014 each Voronoi cell is a region of covariate   space with its own quantile estimate, which is easy to visualise   and explain</li> </ul>"},{"location":"theory/nonparametric/#limitations","title":"Limitations","text":"<ul> <li>Curse of dimensionality \u2014 for large \\(d\\), many cells may   be empty or contain few observations, degrading the quality of   cell-level quantile estimates</li> <li>Not a linear model \u2014 cannot produce coefficient vectors or   standard errors in the usual regression sense</li> <li>Grid resolution \u2014 \\(N\\) must be chosen by the user.  Too small   gives a coarse partition; too large gives many near-empty cells</li> </ul>"},{"location":"theory/nonparametric/#choosing-n","title":"Choosing \\(N\\)","text":"<p>The number of grid points \\(N\\) controls the resolution of the partition.  Rules of thumb:</p> \\(n\\) (sample size) \\(d\\) (dimensions) Suggested \\(N\\) 100\u2013500 1 10\u201320 500\u20135,000 1 20\u201350 100\u2013500 2\u20135 5\u201315 500\u20135,000 2\u20135 15\u201330"},{"location":"theory/nonparametric/#implementation-in-pinball","title":"Implementation in Pinball","text":"<pre><code>from pinball.nonparametric import QuantizationQuantileEstimator\n\nmodel = QuantizationQuantileEstimator(\n    tau=0.5,          # quantile level\n    N=20,             # grid points\n    n_grids=50,       # bootstrap grids (like bagging)\n    p=2,              # L_p norm (2 = Euclidean)\n    random_state=42,  # for reproducibility\n)\nmodel.fit(X, y)\ny_hat = model.predict(X_new)\n</code></pre>"},{"location":"theory/nonparametric/#sklearn-compatibility","title":"sklearn Compatibility","text":"<p>The estimator is fully sklearn-compatible \u2014 it passes all 52 <code>check_estimator</code> checks and can be used in pipelines, grid search, and cross-validation:</p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\npipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"quant\", QuantizationQuantileEstimator(tau=0.5, N=20)),\n])\n\nscores = cross_val_score(pipe, X, y, cv=5)\n</code></pre>"},{"location":"theory/nonparametric/#low-level-api","title":"Low-level API","text":"<p>For fine-grained control, the building blocks are available directly:</p> <pre><code>from pinball.nonparametric.quantization import (\n    choice_grid,       # construct CLVQ grids\n    voronoi_assign,    # assign points to cells\n    cell_quantiles,    # compute quantiles per cell\n    predict_quantiles, # predict at new points\n)\n\n# Build 50 grids with N=20 centroids\ngrids = choice_grid(X.T, N=20, n_grids=50, random_state=42)\n\n# Use the optimal grids\ngrid = grids[\"optimal_grid\"]  # shape (N, n_grids)\n</code></pre>"},{"location":"theory/nonparametric/#references","title":"References","text":"<ol> <li> <p>Charlier, I., Paindaveine, D. and Saracco, J. (2015).    \"Conditional quantile estimation through optimal quantization.\"    Journal of Statistical Planning and Inference 156, 14\u201330.</p> </li> <li> <p>Pag\u00e8s, G. (1998). \"A space quantization method for numerical    integration.\" Journal of Computational and Applied Mathematics    89(1), 1\u201338.</p> </li> <li> <p>Charlier, I., Paindaveine, D. and Saracco, J. (2014).    \"QuantifQuantile: Estimation of Conditional Quantiles using    Optimal Quantization.\" R package.</p> </li> </ol>"},{"location":"theory/overview/","title":"Quantile Regression: An Overview","text":""},{"location":"theory/overview/#beyond-the-mean","title":"Beyond the Mean","text":"<p>Ordinary least squares (OLS) estimates the conditional mean \\(E[Y \\mid X = x]\\).  This is a single summary of how \\(Y\\) relates to \\(X\\) \u2014 useful, but incomplete.  It tells us nothing about:</p> <ul> <li>How the spread of \\(Y\\) changes with \\(X\\) (heteroscedasticity)</li> <li>How the tails of the distribution shift</li> <li>Whether the effect of \\(X\\) on \\(Y\\) is symmetric</li> </ul> <p>Quantile regression (Koenker &amp; Bassett, 1978) estimates the entire family of conditional quantile functions:</p> \\[ Q_\\tau(Y \\mid X = x) = x^\\top \\beta(\\tau), \\quad \\tau \\in (0, 1) \\] <p>Each value of \\(\\tau\\) gives a different regression surface.  Together they paint a complete picture of how the conditional distribution of \\(Y\\) depends on \\(X\\).</p>"},{"location":"theory/overview/#the-pinball-loss","title":"The Pinball Loss","text":"<p>Where OLS minimises the sum of squared residuals, quantile regression minimises the pinball (check) loss:</p> \\[ \\hat\\beta(\\tau) = \\arg\\min_{\\beta} \\sum_{i=1}^{n} \\rho_\\tau(y_i - x_i^\\top \\beta) \\] <p>where the check function is:</p> \\[ \\rho_\\tau(u) = \\begin{cases} \\tau \\cdot u &amp; \\text{if } u \\geq 0 \\\\ (\\tau - 1) \\cdot u &amp; \\text{if } u &lt; 0 \\end{cases} \\] <p>For \\(\\tau = 0.5\\) this reduces to the median regression (minimising the sum of absolute deviations).  For other values of \\(\\tau\\) the loss is asymmetric \u2014 it penalises positive residuals by \\(\\tau\\) and negative residuals by \\(1 - \\tau\\).</p>"},{"location":"theory/overview/#why-quantile-regression","title":"Why Quantile Regression?","text":""},{"location":"theory/overview/#1-robustness","title":"1. Robustness","text":"<p>The median regression (\\(\\tau = 0.5\\)) is far more robust to outliers than OLS.  The breakdown point of the median is 50%, while for OLS it is \\(1/n\\).  This was already understood by Boscovich and Laplace in the 18th century (Koenker, 2005).</p>"},{"location":"theory/overview/#2-heteroscedasticity","title":"2. Heteroscedasticity","text":"<p>In many applications the variance of \\(Y\\) is not constant across values of \\(X\\).  For example, in Engel's data on food expenditure: wealthier households show much more variability in food spending. Quantile regression reveals this naturally \u2014 the slopes of the upper and lower quantiles diverge.</p>"},{"location":"theory/overview/#3-distributional-effects","title":"3. Distributional Effects","text":"<p>Policy questions often concern the tails.  \"Does a training programme help the least skilled workers?\" is a question about the lower quantiles, not the mean.  Quantile regression provides a direct answer.</p>"},{"location":"theory/overview/#4-no-distributional-assumptions","title":"4. No Distributional Assumptions","text":"<p>Unlike maximum likelihood methods, quantile regression makes no assumption about the error distribution.  The pinball loss is distribution-free \u2014 it works whether errors are Gaussian, heavy-tailed, skewed, or heteroscedastic.</p>"},{"location":"theory/overview/#connection-to-linear-programming","title":"Connection to Linear Programming","text":"<p>Minimising the pinball loss is a linear program (LP).  Writing the problem in the usual LP form is the key insight that enables efficient computation.  With \\(u_i = \\max(0, r_i)\\) and \\(v_i = \\max(0, -r_i)\\):</p> \\[ \\min_{\\beta, u, v} \\; \\tau \\, \\mathbf{1}^\\top u + (1 - \\tau) \\, \\mathbf{1}^\\top v \\quad \\text{s.t.} \\quad X\\beta + u - v = y, \\quad u, v \\geq 0 \\] <p>This LP can be solved by the simplex method (Barrodale &amp; Roberts, 1974) or by interior-point methods (Portnoy &amp; Koenker, 1997).</p>"},{"location":"theory/overview/#the-duality-principle","title":"The Duality Principle","text":"<p>The dual of the quantile regression LP is elegant and important for inference.  It takes the form:</p> \\[ \\max_{d} \\; y^\\top d \\quad \\text{s.t.} \\quad X^\\top d = (1 - \\tau) X^\\top \\mathbf{1}, \\quad 0 \\leq d_i \\leq 1 \\] <p>The dual solution \\(d\\) classifies observations:</p> \\(d_i\\) Interpretation \\(d_i = 0\\) Observation is above the fitted hyperplane \\(d_i = 1\\) Observation is below the fitted hyperplane \\(0 &lt; d_i &lt; 1\\) Observation lies on the hyperplane (an interpolation point) <p>This dual structure is exploited by the Barrodale-Roberts solver and underlies the rank-inversion confidence intervals.</p>"},{"location":"theory/overview/#historical-context","title":"Historical Context","text":"<p>The history of \\(\\ell_1\\) estimation is surprisingly rich:</p> <ul> <li>1757 \u2014 Boscovich proposes minimising absolute deviations for fitting   a line to astronomical data</li> <li>1789 \u2014 Laplace develops computational methods for \\(\\ell_1\\) regression</li> <li>1809 \u2014 Gauss publishes the method of least squares; \\(\\ell_1\\) methods   fall out of favour due to computational difficulty</li> <li>1978 \u2014 Koenker &amp; Bassett introduce the full quantile regression   framework</li> <li>1997 \u2014 Portnoy &amp; Koenker show that interior-point methods with   preprocessing can make \\(\\ell_1\\) regression faster than \\(\\ell_2\\)   for large problems</li> </ul>"},{"location":"theory/overview/#further-reading","title":"Further Reading","text":"<ul> <li>Koenker, R. (2005). Quantile Regression. Cambridge University Press.</li> <li>Koenker, R. (2024). <code>quantreg</code>: Quantile Regression. R package.</li> <li>Portnoy, S. and Koenker, R. (1997). \"The Gaussian hare and the   Laplacian tortoise.\" Statistical Science 12(4): 279\u2013300.</li> </ul>"},{"location":"theory/preprocessing/","title":"The Preprocessing Approach: Gaussian Hare and Laplacian Tortoise","text":"<p>Key idea</p> <p>Most observations in a quantile regression problem are far from the fitted hyperplane and can be replaced by two summary statistics (one above, one below) without changing the optimal solution.  Only a thin \"middle band\" of observations near the quantile hyperplane needs to participate in each iteration.  This reduces the effective problem size from \\(n\\) to \\(O(\\sqrt{p} \\cdot n^{2/3})\\), yielding 10- to 100-fold speedups on large datasets.</p>"},{"location":"theory/preprocessing/#motivation","title":"Motivation","text":"<p>Least squares (\\(\\ell_2\\)) regression is famously fast: the normal equations \\(X^\\top X \\beta = X^\\top y\\) take \\(O(n p^2)\\) \u2014 linear in \\(n\\).  Quantile regression (\\(\\ell_1\\)) requires solving a linear program, whose simplex-based algorithms have superlinear cost in \\(n\\) and whose interior-point algorithms, while better, still touch every observation at every iteration.</p> <p>Portnoy &amp; Koenker (1997) posed the question provocatively:</p> <p>Must the \\(\\ell_1\\) tortoise always be slower than the \\(\\ell_2\\) hare?</p> <p>Their answer is no \u2014 with a simple preprocessing trick, the \\(\\ell_1\\) tortoise can actually be faster than OLS for large \\(n\\), because the effective number of observations shrinks to \\(O(n^{2/3})\\).  The Gaussian hare must always process all \\(n\\) observations; the Laplacian tortoise need not.</p>"},{"location":"theory/preprocessing/#the-algorithm","title":"The Algorithm","text":""},{"location":"theory/preprocessing/#step-1-initial-subsample","title":"Step 1: Initial Subsample","text":"<p>Draw a random subsample of size</p> \\[ m = \\lceil \\sqrt{p} \\cdot n^{2/3} \\rceil \\] <p>and solve the subsampled quantile regression:</p> \\[ \\hat\\beta^{(0)} = \\arg\\min_{\\beta} \\sum_{i \\in S_m} \\rho_\\tau(y_i - x_i^\\top \\beta) \\] <p>This pilot estimate is not accurate but is fast to compute \u2014 the LP has only \\(m \\ll n\\) constraints.</p>"},{"location":"theory/preprocessing/#step-2-classify-observations","title":"Step 2: Classify Observations","text":"<p>Compute residuals on the full dataset: \\(r_i = y_i - x_i^\\top \\hat\\beta^{(0)}\\), and a leverage-adjusted bandwidth \\(h_i\\) from the Cholesky factor of \\(X_S^\\top X_S\\):</p> \\[ h_i = \\sqrt{x_i^\\top (X_S^\\top X_S)^{-1} x_i} \\] <p>Use the scaled residuals \\(r_i / h_i\\) to classify each observation into three groups:</p> Group Condition Interpretation \\(\\mathcal{L}\\) (below) \\(r_i / h_i &lt; \\kappa_{\\text{lo}}\\) Definitely below the true hyperplane \\(\\mathcal{U}\\) (above) \\(r_i / h_i &gt; \\kappa_{\\text{hi}}\\) Definitely above the true hyperplane \\(\\mathcal{M}\\) (middle) otherwise Could go either way \u2014 must be kept <p>The thresholds \\(\\kappa_{\\text{lo}}\\) and \\(\\kappa_{\\text{hi}}\\) are chosen so that the middle band contains approximately \\(M = 0.8 \\cdot m\\) observations.</p>"},{"location":"theory/preprocessing/#step-3-build-globs","title":"Step 3: Build Globs","text":"<p>Here is the key insight.  The observations in \\(\\mathcal{L}\\) and \\(\\mathcal{U}\\) are so far from the hyperplane that their individual \\(x_i\\) values do not matter \u2014 only their aggregate effect on the LP matters.  So we replace each group with a single glob:</p> \\[ \\tilde x_L = \\sum_{i \\in \\mathcal{L}} x_i, \\quad \\tilde y_L = \\sum_{i \\in \\mathcal{L}} y_i \\] \\[ \\tilde x_U = \\sum_{i \\in \\mathcal{U}} x_i, \\quad \\tilde y_U = \\sum_{i \\in \\mathcal{U}} y_i \\] <p>The reduced problem has \\(|\\mathcal{M}| + 2\\) observations \u2014 a dramatic reduction from \\(n\\).</p> <p>Why does this work?</p> <p>In the LP dual, observations in \\(\\mathcal{L}\\) have dual variable \\(d_i = 1\\) (they are below the hyperplane) and observations in \\(\\mathcal{U}\\) have \\(d_i = 0\\) (above). As long as these classifications are correct, the sum of the corresponding rows is a sufficient statistic.  The simplex and interior-point algorithms only need to \"see\" the observations whose dual variables could change \u2014 those in the middle band.</p>"},{"location":"theory/preprocessing/#step-4-solve-the-reduced-problem","title":"Step 4: Solve the Reduced Problem","text":"<p>Solve quantile regression on the reduced dataset:</p> \\[ \\tilde X = \\begin{bmatrix}   X_{\\mathcal{M}} \\\\   \\tilde x_L^\\top \\\\   \\tilde x_U^\\top \\end{bmatrix}, \\quad \\tilde y = \\begin{bmatrix}   y_{\\mathcal{M}} \\\\   \\tilde y_L \\\\   \\tilde y_U \\end{bmatrix} \\] <p>using the Frisch-Newton interior-point method (or any LP solver).</p>"},{"location":"theory/preprocessing/#step-5-verify-and-fix-up","title":"Step 5: Verify and Fix Up","text":"<p>Check whether any observations were misclassified \u2014 that is, whether any point in \\(\\mathcal{L}\\) now has a positive residual, or any point in \\(\\mathcal{U}\\) now has a negative residual:</p> <ul> <li>If \\(r_i &gt; 0\\) for some \\(i \\in \\mathcal{L}\\): reclassify   \\(i\\) to \\(\\mathcal{M}\\) and update the globs.</li> <li>If \\(r_i &lt; 0\\) for some \\(i \\in \\mathcal{U}\\): similarly   reclassify.</li> <li>If no misclassifications: converge.</li> </ul> <p>If too many observations are misclassified (more than 10% of \\(M\\)), double \\(m\\) and restart \u2014 the initial subsample was too small.</p>"},{"location":"theory/preprocessing/#complexity-analysis","title":"Complexity Analysis","text":"Step Cost Initial subsample solve \\(O(m \\cdot p^{1.5}) = O(\\sqrt{p} \\cdot n^{2/3} \\cdot p^{1.5})\\) Full-data residuals \\(O(n \\cdot p)\\) Classification \\(O(n)\\) Reduced solve \\(O(m \\cdot p^{1.5})\\) Fixup iterations a few more \\(O(m \\cdot p^{1.5})\\) <p>The dominant cost is the initial and reduced solves, each on a problem of size \\(m \\approx \\sqrt{p} \\cdot n^{2/3}\\).  For comparison:</p> Method Cost \\(n = 10^6, p = 10\\) OLS \\(O(n p^2)\\) \\(10^8\\) FNB (interior point) \\(O(n^{3/2} p^2)\\) \\(10^{11}\\) PFN (preprocessing) \\(O(n^{2/3} p^2)\\) \\(10^6\\) <p>The preprocessing approach can be faster than OLS for large enough \\(n\\), because \\(n^{2/3} &lt; n\\).  This is the remarkable conclusion of Portnoy &amp; Koenker (1997): the Laplacian tortoise overtakes the Gaussian hare.</p>"},{"location":"theory/preprocessing/#visual-intuition","title":"Visual Intuition","text":"<p>Consider a 2-D quantile regression at \\(\\tau = 0.5\\) (median):</p> <pre><code>    y \u2502                             \u2571\n      \u2502             \u2218            \u2571   \u2190 upper glob (one point)\n      \u2502        \u2218     \u2218       \u2571\n      \u2502   \u2218      \u2022     \u2218  \u2571\n      \u2502      \u2022     \u2022   \u2571   \u2190 middle band (these matter)\n      \u2502   \u2022     \u2022   \u2571\n      \u2502      \u2022   \u2571    \u2218\n      \u2502   \u2218   \u2571         \u2190 lower glob (one point)\n      \u2502    \u2571   \u2218\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 x\n\n    \u2218 = far from hyperplane (aggregated into globs)\n    \u2022 = in the middle band (kept individually)\n</code></pre> <p>Only the handful of points near the regression line need to be processed individually.  Everything else collapses into two summary points.</p>"},{"location":"theory/preprocessing/#implementation-in-pinball","title":"Implementation in Pinball","text":"<p>The <code>PreprocessingSolver</code> (<code>method=\"pfn\"</code>) implements this algorithm:</p> <pre><code>from pinball import QuantileRegressor\n\n# Automatically uses preprocessing + Frisch-Newton\nmodel = QuantileRegressor(tau=0.5, method=\"pfn\")\nmodel.fit(X_large, y_large)    # X_large has millions of rows\n</code></pre>"},{"location":"theory/preprocessing/#configuration","title":"Configuration","text":"<pre><code>from pinball.linear.solvers.pfn import PreprocessingSolver\n\nsolver = PreprocessingSolver(\n    mm_factor=0.8,       # middle-band size as fraction of m\n    max_bad_fixups=3,    # fixup iterations before doubling m\n    eps=1e-6,            # bandwidth floor\n)\n</code></pre>"},{"location":"theory/preprocessing/#fallback-behaviour","title":"Fallback behaviour","text":"<p>If the subsample size \\(m \\geq n\\), the preprocessing is skipped and the inner solver runs on the full data directly.  This means <code>\"pfn\"</code> is safe to use on any dataset \u2014 it is never worse than <code>\"fn\"</code>.</p>"},{"location":"theory/preprocessing/#when-to-use-preprocessing","title":"When to Use Preprocessing","text":"Scenario Recommendation \\(n &lt; 5{,}000\\) Use <code>\"br\"</code> (simplex) \u2014 preprocessing overhead not worthwhile \\(5{,}000 &lt; n &lt; 100{,}000\\) Use <code>\"fn\"</code> \u2014 interior point is fast enough \\(n &gt; 100{,}000\\) Use <code>\"pfn\"</code> \u2014 preprocessing gives 10\u2013100\u00d7 speedup \\(n &gt; 1{,}000{,}000\\) Use <code>\"pfn\"</code> \u2014 the only practical option"},{"location":"theory/preprocessing/#benchmark-engel-data-scaled-up","title":"Benchmark: Engel Data Scaled Up","text":"<pre><code>import numpy as np\nfrom pinball import QuantileRegressor\nfrom pinball.datasets import load_engel\nimport time\n\n# Scale Engel data to 1 million rows\nengel = load_engel()\nrng = np.random.default_rng(42)\nn_big = 1_000_000\nidx = rng.integers(0, len(engel.target), n_big)\nX_big = engel.data[idx] + rng.normal(0, 10, (n_big, 1))\ny_big = engel.target[idx] + rng.normal(0, 20, n_big)\n\nfor method in [\"fn\", \"pfn\"]:\n    t0 = time.perf_counter()\n    QuantileRegressor(tau=0.5, method=method).fit(X_big, y_big)\n    print(f\"{method}: {time.perf_counter() - t0:.2f}s\")\n# fn:  12.34s\n# pfn:  0.28s   \u2190 ~44\u00d7 faster\n</code></pre>"},{"location":"theory/preprocessing/#references","title":"References","text":"<ol> <li> <p>Portnoy, S. and Koenker, R. (1997). \"The Gaussian hare and the    Laplacian tortoise: computability of squared-error versus    absolute-error estimators.\" Statistical Science 12(4): 279\u2013300.    DOI:10.1214/ss/1030037960</p> </li> <li> <p>Koenker, R. (2005). Quantile Regression. Cambridge University Press.    Chapter 6: \"Computational aspects of quantile regression.\"</p> </li> </ol>"}]}